{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9dce5f7-6714-451a-a095-e1a5a0871306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ModelExporter:\n",
    "    \"\"\"\n",
    "    A utility class to export a PyTorch model to an ONNX file.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name):\n",
    "        \"\"\"\n",
    "        Initialize the exporter with a model name.\n",
    "        Args:\n",
    "            model_name (str): The name of the model. This will be used as the ONNX file name.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def export_to_onnx(self, model, dummy_input):\n",
    "        \"\"\"\n",
    "        Exports the given PyTorch model to an ONNX file.\n",
    "        Args:\n",
    "            model (torch.nn.Module): The PyTorch model to be exported.\n",
    "            dummy_input (torch.Tensor): A dummy input tensor that matches the input shape of the model.\n",
    "        \"\"\"\n",
    "        # Generate the ONNX file name using the model name\n",
    "        onnx_file_name = f\"model_onnx/{self.model_name}.onnx\"\n",
    "\n",
    "        # Export the model to ONNX format\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            onnx_file_name,           # Name of the output ONNX file\n",
    "            export_params=True,       # Include the trained parameters in the exported file\n",
    "            opset_version=11,         # Specify the ONNX opset version\n",
    "            do_constant_folding=True, # Perform constant folding optimization\n",
    "            input_names=[\"input\"],    # Name of the input tensor\n",
    "            output_names=[\"output\"]   # Name of the output tensor\n",
    "        )\n",
    "        print(f\"The ONNX file has been saved as '{onnx_file_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a141462-e8bc-4727-8a7f-2df88e1dba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CustomReLU(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Custom implementation of the ReLU activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CustomReLU, self).__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Applies the ReLU function element-wise: max(0, x).\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor with ReLU applied\n",
    "        \"\"\"\n",
    "        return torch.maximum(x, torch.zeros_like(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73ef2b6b-0f07-4601-a602-44174a8eb473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ConvolutionalLayer:\n",
    "    \"\"\"\n",
    "    A utility class to create a convolutional layer with a custom ReLU activation.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def create(in_channels: int, out_channels: int, kernel_size: int, stride: int, padding: int) -> torch.nn.Sequential:\n",
    "        \"\"\"\n",
    "        Creates a convolutional layer with a custom ReLU activation.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels\n",
    "            out_channels (int): Number of output channels\n",
    "            kernel_size (int): Size of the convolution kernel\n",
    "            stride (int): Stride of the convolution\n",
    "            padding (int): Padding added to the input\n",
    "        Returns:\n",
    "            torch.nn.Sequential: A sequential layer containing Conv2d and CustomReLU(+Batch Normalization)\n",
    "        \"\"\"\n",
    "        return torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            CustomReLU()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2efcacde-ae2d-4e7d-bc5c-f35a806c0843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class FullyConnectedLayer:\n",
    "    \"\"\"\n",
    "    A utility class to create Fully Connected (FC) layers with Xavier initialization.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def Dense(input_dim: int, output_dim: int) -> torch.nn.Linear:\n",
    "        \"\"\"\n",
    "        Creates an FC layer and applies Xavier initialization.\n",
    "        Args:\n",
    "            input_dim (int): Number of input features\n",
    "            output_dim (int): Number of output features\n",
    "        Returns:\n",
    "            torch.nn.Linear: Initialized Fully-Connected layer\n",
    "        \"\"\"\n",
    "        layer = torch.nn.Linear(input_dim, output_dim, bias=True)\n",
    "        torch.nn.init.xavier_uniform_(layer.weight)\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "165f11a9-0b07-410b-92d9-38795e8d27f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Stem(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the Stem layer(Base on Figure 14) introduced by Inception-ResNet-V1\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> torch.nn.Sequential:\n",
    "        \"\"\"\n",
    "        Creates a Inception layer with a custom ReLU activation Function.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels\n",
    "            out_channels (int): Number of output channels\n",
    "        Returns:\n",
    "            torch.nn.Sequential: A sequential block representing the Stem Layer\n",
    "        \"\"\"\n",
    "        super(Stem, self).__init__()\n",
    "\n",
    "        self.step1 = ConvolutionalLayer.create(3, 32, 3, 2, 0)\n",
    "\n",
    "        self.step2 = ConvolutionalLayer.create(32, 32, 3, 1, 0)\n",
    "\n",
    "        self.step3 = ConvolutionalLayer.create(32, 64, 3, 1, 1)\n",
    "\n",
    "        self.step4 = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "\n",
    "        self.step5 = ConvolutionalLayer.create(64, 80, 1, 1, 0)\n",
    "\n",
    "        self.step6 = ConvolutionalLayer.create(80, 192, 3, 1, 0)\n",
    "\n",
    "        self.step7 = ConvolutionalLayer.create(192, out_channels, 3, 2, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the Inception-ResNet-V1 layer.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width)\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after passing through all branches of the Stem layer\n",
    "        \"\"\"\n",
    "        x = self.step1(x)\n",
    "        x = self.step2(x)\n",
    "        x = self.step3(x)\n",
    "        x = self.step4(x)\n",
    "        x = self.step5(x)\n",
    "        x = self.step6(x)\n",
    "        x = self.step7(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebe6c5db-b839-46a5-8caa-a081eeb90972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Inception_A(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the Inception_A layer(Base on Figure 10) introduced by Inception-ResNet-V1\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> torch.nn.Sequential:\n",
    "        \"\"\"\n",
    "        Creates a Inception layer with a custom ReLU activation Function.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels\n",
    "            out_channels (int): Number of output channels\n",
    "            The value of the out_channels parameter must always be a multiple of 4.\n",
    "        Returns:\n",
    "            torch.nn.Sequential: A sequential block representing the Inception_A Layer\n",
    "        \"\"\"\n",
    "        super(Inception_A, self).__init__()\n",
    "\n",
    "        self.branch1_1 = ConvolutionalLayer.create(in_channels, 32, 1, 1, 0)\n",
    "\n",
    "        self.branch2_1 = ConvolutionalLayer.create(in_channels, 32, 1, 1, 0)\n",
    "        self.branch2_2 = ConvolutionalLayer.create(32, 32, 3, 1, 1)\n",
    "\n",
    "        self.branch3_1 = ConvolutionalLayer.create(in_channels, 32, 1, 1, 0)\n",
    "        self.branch3_2 = ConvolutionalLayer.create(32, 32, 3, 1, 1)\n",
    "        self.branch3_3 = ConvolutionalLayer.create(32, 32, 3, 1, 1)\n",
    "\n",
    "        self.step2 = torch.nn.Conv2d(32 + 32 + 32, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.step3 = CustomReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the Inception-ResNet-V1 layer.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width)\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after passing through all branches of the Inception_A layer\n",
    "        \"\"\"\n",
    "        x_b1 = self.branch1_1(x)\n",
    "\n",
    "        x_b2 = self.branch2_1(x)\n",
    "        x_b2 = self.branch2_2(x_b2)\n",
    "\n",
    "        x_b3 = self.branch3_1(x)\n",
    "        x_b3 = self.branch3_2(x_b3)\n",
    "        x_b3 = self.branch3_3(x_b3)\n",
    "\n",
    "        x_step = torch.cat([x_b1, x_b2, x_b3], dim=1)\n",
    "        x_step = self.step2(x_step)\n",
    "\n",
    "        x += x_step\n",
    "        x = self.step3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f391c851-77a9-4366-91e1-03d99ea0f8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Inception_B(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the Inception_B layer(Base on Figure 11) introduced by Inception-ResNet-V1\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> torch.nn.Sequential:\n",
    "        \"\"\"\n",
    "        Creates a Inception layer with a custom ReLU activation Function.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels\n",
    "            out_channels (int): Number of output channels\n",
    "        Returns:\n",
    "            torch.nn.Sequential: A sequential block representing the Inception_B Layer\n",
    "        \"\"\"\n",
    "        super(Inception_B, self).__init__()\n",
    "\n",
    "        self.branch1_1 = ConvolutionalLayer.create(in_channels, 128, 1, 1, 0)\n",
    "\n",
    "        self.branch2_1 = ConvolutionalLayer.create(in_channels, 128, 1, 1, 0)\n",
    "        self.branch2_2 = ConvolutionalLayer.create(128, 128, (1, 7), 1, (0, 3))\n",
    "        self.branch2_3 = ConvolutionalLayer.create(128, 128, (7, 1), 1, (3, 0))\n",
    "\n",
    "        self.step2 = torch.nn.Conv2d(128 + 128, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.step3 = CustomReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the Inception-ResNet-V1 layer.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width)\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after passing through all branches of the Inception_B layer\n",
    "        \"\"\"\n",
    "        x_b1 = self.branch1_1(x)\n",
    "\n",
    "        x_b2 = self.branch2_1(x)\n",
    "        x_b2 = self.branch2_2(x_b2)\n",
    "        x_b2 = self.branch2_3(x_b2)\n",
    "\n",
    "        x_step = torch.cat([x_b1, x_b2], dim=1)\n",
    "        x_step = self.step2(x_step)\n",
    "\n",
    "        x += x_step\n",
    "        x = self.step3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cba988ff-f4c4-47a6-889e-3fc38889b665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Inception_C(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the Inception_C layer(Base on Figure 13) introduced by Inception-ResNet-V1\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> torch.nn.Sequential:\n",
    "        \"\"\"\n",
    "        Creates a Inception layer with a custom ReLU activation Function.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels\n",
    "            out_channels (int): Number of output channels\n",
    "        Returns:\n",
    "            torch.nn.Sequential: A sequential block representing the Inception_C Layer\n",
    "        \"\"\"\n",
    "        super(Inception_C, self).__init__()\n",
    "\n",
    "        self.branch1_1 = ConvolutionalLayer.create(in_channels, 192, 1, 1, 0)\n",
    "\n",
    "        self.branch2_1 = ConvolutionalLayer.create(in_channels, 192, 1, 1, 0)\n",
    "        self.branch2_2 = ConvolutionalLayer.create(192, 192, (1, 3), 1, (0, 1))\n",
    "        self.branch2_3 = ConvolutionalLayer.create(192, 192, (3, 1), 1, (1, 0))\n",
    "\n",
    "        self.step2 = torch.nn.Conv2d(192 + 192, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.step3 = CustomReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the Inception-ResNet-V1 layer.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width)\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after passing through all branches of the Inception_C layer\n",
    "        \"\"\"\n",
    "        x_b1 = self.branch1_1(x)\n",
    "\n",
    "        x_b2 = self.branch2_1(x)\n",
    "        x_b2 = self.branch2_2(x_b2)\n",
    "        x_b2 = self.branch2_3(x_b2)\n",
    "\n",
    "        x_step = torch.cat([x_b1, x_b2], dim=1)\n",
    "        x_step = self.step2(x_step)\n",
    "\n",
    "        x += x_step\n",
    "        x = self.step3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "615f70b0-5174-4081-9d13-7e5a0ba875e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Reduction_A(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the Reduction_A layer(Base on Figure 7) introduced by Inception V4, Inception-ResNet-V1 and Inception-ResNet-V2\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, n: int, k: int, l: int, m: int) -> torch.nn.Sequential:\n",
    "        \"\"\"\n",
    "        Creates a Inception layer with a custom ReLU activation Function.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels\n",
    "            n (int): The number of filters to output after the 3x3 Convolutions operation in Branch2. (In Paper, 384)\n",
    "            k (int): The number of filters to output after the 1x1 Convolutions operation in Branch3. (In Paper, 192)\n",
    "            l (int): The number of filters to output after the 3x3 Convolutions operation in Branch3. (In Paper, 192)\n",
    "            m (int): The number of filters to output after the 3x3 Convolutions operation(with stride=2) in Branch3. (In Paper, 256)\n",
    "        Returns:\n",
    "            torch.nn.Sequential: A sequential block representing the Reduction_A Layer\n",
    "        \"\"\"\n",
    "        super(Reduction_A, self).__init__()\n",
    "\n",
    "        self.branch1_1 = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "\n",
    "        self.branch2_1 = ConvolutionalLayer.create(in_channels, n, 3, 2, 0)\n",
    "\n",
    "        self.branch3_1 = ConvolutionalLayer.create(in_channels, k, 1, 1, 0)\n",
    "        self.branch3_2 = ConvolutionalLayer.create(k, l, 3, 1, 1)\n",
    "        self.branch3_3 = ConvolutionalLayer.create(l, m, 3, 2, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the Inception V4, Inception-ResNet-V1 and Inception-ResNet-V2 layer.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width)\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after passing through all branches of the Reduction_A layer\n",
    "        \"\"\"\n",
    "        x_b1 = self.branch1_1(x)\n",
    "\n",
    "        x_b2 = self.branch2_1(x)\n",
    "\n",
    "        x_b3 = self.branch3_1(x)\n",
    "        x_b3 = self.branch3_2(x_b3)\n",
    "        x_b3 = self.branch3_3(x_b3)\n",
    "\n",
    "        x = torch.cat([x_b1, x_b2, x_b3], dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28559752-8fd2-43fc-99cf-42deed9b5697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Reduction_B(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the Reduction_B layer(Base on Figure 12) introduced by Inception-ResNet-V1\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> torch.nn.Sequential:\n",
    "        \"\"\"\n",
    "        Creates a Inception layer with a custom ReLU activation Function.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels\n",
    "            out_channels (int): Number of output channels\n",
    "            The value of the out_channels parameter must always be a multiple of 7\n",
    "        Returns:\n",
    "            torch.nn.Sequential: A sequential block representing the Reduction_B Layer\n",
    "        \"\"\"\n",
    "        super(Reduction_B, self).__init__()\n",
    "\n",
    "        self.branch1_1 = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "\n",
    "        self.branch2_1 = ConvolutionalLayer.create(in_channels, 256, 1, 1, 0)\n",
    "        self.branch2_2 = ConvolutionalLayer.create(256, ((out_channels - in_channels) // 7) * 3, 3, 2, 0)\n",
    "\n",
    "        self.branch3_1 = ConvolutionalLayer.create(in_channels, 256, 1, 1, 0)\n",
    "        self.branch3_2 = ConvolutionalLayer.create(256, ((out_channels - in_channels) // 7) * 2, 3, 2, 0)\n",
    "\n",
    "        self.branch4_1 = ConvolutionalLayer.create(in_channels, 256, 1, 1, 0)\n",
    "        self.branch4_2 = ConvolutionalLayer.create(256, 256, 3, 1, 1)\n",
    "        self.branch4_3 = ConvolutionalLayer.create(256, ((out_channels - in_channels) // 7) * 2, 3, 2, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the Inception-ResNet-V1 layer.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width)\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after passing through all branches of the Reduction_B layer\n",
    "        \"\"\"\n",
    "        x_b1 = self.branch1_1(x)\n",
    "\n",
    "        x_b2 = self.branch2_1(x)\n",
    "        x_b2 = self.branch2_2(x_b2)\n",
    "\n",
    "        x_b3 = self.branch3_1(x)\n",
    "        x_b3 = self.branch3_2(x_b3)\n",
    "\n",
    "        x_b4 = self.branch4_1(x)\n",
    "        x_b4 = self.branch4_2(x_b4)\n",
    "        x_b4 = self.branch4_3(x_b4)\n",
    "\n",
    "        x = torch.cat([x_b1, x_b2, x_b3, x_b4], dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05e28546-0cde-424e-b1b0-ee510af9e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CustomInceptionResNet_V1(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the introduced by Inception-ResNet-V1 model.\n",
    "    Input: Image tensor (batch_size, 3, 299, 299)\n",
    "    Output: Class scores (batch_size, 1000)\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_rate=0.8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dropout_rate (float): Dropout Rate = 0.8(Base on Paper)\n",
    "        \"\"\"\n",
    "        super(CustomInceptionResNet_V1, self).__init__()\n",
    "\n",
    "        # Convolutional and pooling layers\n",
    "        self.layer1 = Stem(3, 256)\n",
    "\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            *[Inception_A(256, 256) for _ in range(5)]\n",
    "        )\n",
    "\n",
    "        self.layer3 = Reduction_A(256, 384, 192, 192, 256)\n",
    "\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            *[Inception_B(896, 896) for _ in range(10)]\n",
    "        )\n",
    "\n",
    "        self.layer5 = Reduction_B(896, 1792)\n",
    "\n",
    "        self.layer6 = torch.nn.Sequential(\n",
    "            *[Inception_C(1792, 1792) for _ in range(5)]\n",
    "        )\n",
    "\n",
    "        self.layer7 = torch.nn.AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
    "        \n",
    "\n",
    "        # Fully Connected layers and dropout\n",
    "        self.layer_drop = torch.nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        self.layer8 = FullyConnectedLayer.Dense(1792, 1000)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input image tensor (batch_size, 3, 299, 299)\n",
    "        Returns:\n",
    "            torch.Tensor: Class scores (batch_size, 1000)\n",
    "        \"\"\"\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "        x = self.layer7(x)\n",
    "        x = self.layer_drop(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.layer8(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dde87c1a-4705-4945-ba54-bb40126f0140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomInceptionResNet_V1\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 149, 149]             896\n",
      "       BatchNorm2d-2         [-1, 32, 149, 149]              64\n",
      "        CustomReLU-3         [-1, 32, 149, 149]               0\n",
      "            Conv2d-4         [-1, 32, 147, 147]           9,248\n",
      "       BatchNorm2d-5         [-1, 32, 147, 147]              64\n",
      "        CustomReLU-6         [-1, 32, 147, 147]               0\n",
      "            Conv2d-7         [-1, 64, 147, 147]          18,496\n",
      "       BatchNorm2d-8         [-1, 64, 147, 147]             128\n",
      "        CustomReLU-9         [-1, 64, 147, 147]               0\n",
      "        MaxPool2d-10           [-1, 64, 73, 73]               0\n",
      "           Conv2d-11           [-1, 80, 73, 73]           5,200\n",
      "      BatchNorm2d-12           [-1, 80, 73, 73]             160\n",
      "       CustomReLU-13           [-1, 80, 73, 73]               0\n",
      "           Conv2d-14          [-1, 192, 71, 71]         138,432\n",
      "      BatchNorm2d-15          [-1, 192, 71, 71]             384\n",
      "       CustomReLU-16          [-1, 192, 71, 71]               0\n",
      "           Conv2d-17          [-1, 256, 35, 35]         442,624\n",
      "      BatchNorm2d-18          [-1, 256, 35, 35]             512\n",
      "       CustomReLU-19          [-1, 256, 35, 35]               0\n",
      "             Stem-20          [-1, 256, 35, 35]               0\n",
      "           Conv2d-21           [-1, 32, 35, 35]           8,224\n",
      "      BatchNorm2d-22           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-23           [-1, 32, 35, 35]               0\n",
      "           Conv2d-24           [-1, 32, 35, 35]           8,224\n",
      "      BatchNorm2d-25           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-26           [-1, 32, 35, 35]               0\n",
      "           Conv2d-27           [-1, 32, 35, 35]           9,248\n",
      "      BatchNorm2d-28           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-29           [-1, 32, 35, 35]               0\n",
      "           Conv2d-30           [-1, 32, 35, 35]           8,224\n",
      "      BatchNorm2d-31           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-32           [-1, 32, 35, 35]               0\n",
      "           Conv2d-33           [-1, 32, 35, 35]           9,248\n",
      "      BatchNorm2d-34           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-35           [-1, 32, 35, 35]               0\n",
      "           Conv2d-36           [-1, 32, 35, 35]           9,248\n",
      "      BatchNorm2d-37           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-38           [-1, 32, 35, 35]               0\n",
      "           Conv2d-39          [-1, 256, 35, 35]          24,832\n",
      "       CustomReLU-40          [-1, 256, 35, 35]               0\n",
      "      Inception_A-41          [-1, 256, 35, 35]               0\n",
      "           Conv2d-42           [-1, 32, 35, 35]           8,224\n",
      "      BatchNorm2d-43           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-44           [-1, 32, 35, 35]               0\n",
      "           Conv2d-45           [-1, 32, 35, 35]           8,224\n",
      "      BatchNorm2d-46           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-47           [-1, 32, 35, 35]               0\n",
      "           Conv2d-48           [-1, 32, 35, 35]           9,248\n",
      "      BatchNorm2d-49           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-50           [-1, 32, 35, 35]               0\n",
      "           Conv2d-51           [-1, 32, 35, 35]           8,224\n",
      "      BatchNorm2d-52           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-53           [-1, 32, 35, 35]               0\n",
      "           Conv2d-54           [-1, 32, 35, 35]           9,248\n",
      "      BatchNorm2d-55           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-56           [-1, 32, 35, 35]               0\n",
      "           Conv2d-57           [-1, 32, 35, 35]           9,248\n",
      "      BatchNorm2d-58           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-59           [-1, 32, 35, 35]               0\n",
      "           Conv2d-60          [-1, 256, 35, 35]          24,832\n",
      "       CustomReLU-61          [-1, 256, 35, 35]               0\n",
      "      Inception_A-62          [-1, 256, 35, 35]               0\n",
      "           Conv2d-63           [-1, 32, 35, 35]           8,224\n",
      "      BatchNorm2d-64           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-65           [-1, 32, 35, 35]               0\n",
      "           Conv2d-66           [-1, 32, 35, 35]           8,224\n",
      "      BatchNorm2d-67           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-68           [-1, 32, 35, 35]               0\n",
      "           Conv2d-69           [-1, 32, 35, 35]           9,248\n",
      "      BatchNorm2d-70           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-71           [-1, 32, 35, 35]               0\n",
      "           Conv2d-72           [-1, 32, 35, 35]           8,224\n",
      "      BatchNorm2d-73           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-74           [-1, 32, 35, 35]               0\n",
      "           Conv2d-75           [-1, 32, 35, 35]           9,248\n",
      "      BatchNorm2d-76           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-77           [-1, 32, 35, 35]               0\n",
      "           Conv2d-78           [-1, 32, 35, 35]           9,248\n",
      "      BatchNorm2d-79           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-80           [-1, 32, 35, 35]               0\n",
      "           Conv2d-81          [-1, 256, 35, 35]          24,832\n",
      "       CustomReLU-82          [-1, 256, 35, 35]               0\n",
      "      Inception_A-83          [-1, 256, 35, 35]               0\n",
      "           Conv2d-84           [-1, 32, 35, 35]           8,224\n",
      "      BatchNorm2d-85           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-86           [-1, 32, 35, 35]               0\n",
      "           Conv2d-87           [-1, 32, 35, 35]           8,224\n",
      "      BatchNorm2d-88           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-89           [-1, 32, 35, 35]               0\n",
      "           Conv2d-90           [-1, 32, 35, 35]           9,248\n",
      "      BatchNorm2d-91           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-92           [-1, 32, 35, 35]               0\n",
      "           Conv2d-93           [-1, 32, 35, 35]           8,224\n",
      "      BatchNorm2d-94           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-95           [-1, 32, 35, 35]               0\n",
      "           Conv2d-96           [-1, 32, 35, 35]           9,248\n",
      "      BatchNorm2d-97           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-98           [-1, 32, 35, 35]               0\n",
      "           Conv2d-99           [-1, 32, 35, 35]           9,248\n",
      "     BatchNorm2d-100           [-1, 32, 35, 35]              64\n",
      "      CustomReLU-101           [-1, 32, 35, 35]               0\n",
      "          Conv2d-102          [-1, 256, 35, 35]          24,832\n",
      "      CustomReLU-103          [-1, 256, 35, 35]               0\n",
      "     Inception_A-104          [-1, 256, 35, 35]               0\n",
      "          Conv2d-105           [-1, 32, 35, 35]           8,224\n",
      "     BatchNorm2d-106           [-1, 32, 35, 35]              64\n",
      "      CustomReLU-107           [-1, 32, 35, 35]               0\n",
      "          Conv2d-108           [-1, 32, 35, 35]           8,224\n",
      "     BatchNorm2d-109           [-1, 32, 35, 35]              64\n",
      "      CustomReLU-110           [-1, 32, 35, 35]               0\n",
      "          Conv2d-111           [-1, 32, 35, 35]           9,248\n",
      "     BatchNorm2d-112           [-1, 32, 35, 35]              64\n",
      "      CustomReLU-113           [-1, 32, 35, 35]               0\n",
      "          Conv2d-114           [-1, 32, 35, 35]           8,224\n",
      "     BatchNorm2d-115           [-1, 32, 35, 35]              64\n",
      "      CustomReLU-116           [-1, 32, 35, 35]               0\n",
      "          Conv2d-117           [-1, 32, 35, 35]           9,248\n",
      "     BatchNorm2d-118           [-1, 32, 35, 35]              64\n",
      "      CustomReLU-119           [-1, 32, 35, 35]               0\n",
      "          Conv2d-120           [-1, 32, 35, 35]           9,248\n",
      "     BatchNorm2d-121           [-1, 32, 35, 35]              64\n",
      "      CustomReLU-122           [-1, 32, 35, 35]               0\n",
      "          Conv2d-123          [-1, 256, 35, 35]          24,832\n",
      "      CustomReLU-124          [-1, 256, 35, 35]               0\n",
      "     Inception_A-125          [-1, 256, 35, 35]               0\n",
      "       MaxPool2d-126          [-1, 256, 17, 17]               0\n",
      "          Conv2d-127          [-1, 384, 17, 17]         885,120\n",
      "     BatchNorm2d-128          [-1, 384, 17, 17]             768\n",
      "      CustomReLU-129          [-1, 384, 17, 17]               0\n",
      "          Conv2d-130          [-1, 192, 35, 35]          49,344\n",
      "     BatchNorm2d-131          [-1, 192, 35, 35]             384\n",
      "      CustomReLU-132          [-1, 192, 35, 35]               0\n",
      "          Conv2d-133          [-1, 192, 35, 35]         331,968\n",
      "     BatchNorm2d-134          [-1, 192, 35, 35]             384\n",
      "      CustomReLU-135          [-1, 192, 35, 35]               0\n",
      "          Conv2d-136          [-1, 256, 17, 17]         442,624\n",
      "     BatchNorm2d-137          [-1, 256, 17, 17]             512\n",
      "      CustomReLU-138          [-1, 256, 17, 17]               0\n",
      "     Reduction_A-139          [-1, 896, 17, 17]               0\n",
      "          Conv2d-140          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-141          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-142          [-1, 128, 17, 17]               0\n",
      "          Conv2d-143          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-144          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-145          [-1, 128, 17, 17]               0\n",
      "          Conv2d-146          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-147          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-148          [-1, 128, 17, 17]               0\n",
      "          Conv2d-149          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-150          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-151          [-1, 128, 17, 17]               0\n",
      "          Conv2d-152          [-1, 896, 17, 17]         230,272\n",
      "      CustomReLU-153          [-1, 896, 17, 17]               0\n",
      "     Inception_B-154          [-1, 896, 17, 17]               0\n",
      "          Conv2d-155          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-156          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-157          [-1, 128, 17, 17]               0\n",
      "          Conv2d-158          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-159          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-160          [-1, 128, 17, 17]               0\n",
      "          Conv2d-161          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-162          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-163          [-1, 128, 17, 17]               0\n",
      "          Conv2d-164          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-165          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-166          [-1, 128, 17, 17]               0\n",
      "          Conv2d-167          [-1, 896, 17, 17]         230,272\n",
      "      CustomReLU-168          [-1, 896, 17, 17]               0\n",
      "     Inception_B-169          [-1, 896, 17, 17]               0\n",
      "          Conv2d-170          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-171          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-172          [-1, 128, 17, 17]               0\n",
      "          Conv2d-173          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-174          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-175          [-1, 128, 17, 17]               0\n",
      "          Conv2d-176          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-177          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-178          [-1, 128, 17, 17]               0\n",
      "          Conv2d-179          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-180          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-181          [-1, 128, 17, 17]               0\n",
      "          Conv2d-182          [-1, 896, 17, 17]         230,272\n",
      "      CustomReLU-183          [-1, 896, 17, 17]               0\n",
      "     Inception_B-184          [-1, 896, 17, 17]               0\n",
      "          Conv2d-185          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-186          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-187          [-1, 128, 17, 17]               0\n",
      "          Conv2d-188          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-189          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-190          [-1, 128, 17, 17]               0\n",
      "          Conv2d-191          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-192          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-193          [-1, 128, 17, 17]               0\n",
      "          Conv2d-194          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-195          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-196          [-1, 128, 17, 17]               0\n",
      "          Conv2d-197          [-1, 896, 17, 17]         230,272\n",
      "      CustomReLU-198          [-1, 896, 17, 17]               0\n",
      "     Inception_B-199          [-1, 896, 17, 17]               0\n",
      "          Conv2d-200          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-201          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-202          [-1, 128, 17, 17]               0\n",
      "          Conv2d-203          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-204          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-205          [-1, 128, 17, 17]               0\n",
      "          Conv2d-206          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-207          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-208          [-1, 128, 17, 17]               0\n",
      "          Conv2d-209          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-210          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-211          [-1, 128, 17, 17]               0\n",
      "          Conv2d-212          [-1, 896, 17, 17]         230,272\n",
      "      CustomReLU-213          [-1, 896, 17, 17]               0\n",
      "     Inception_B-214          [-1, 896, 17, 17]               0\n",
      "          Conv2d-215          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-216          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-217          [-1, 128, 17, 17]               0\n",
      "          Conv2d-218          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-219          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-220          [-1, 128, 17, 17]               0\n",
      "          Conv2d-221          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-222          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-223          [-1, 128, 17, 17]               0\n",
      "          Conv2d-224          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-225          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-226          [-1, 128, 17, 17]               0\n",
      "          Conv2d-227          [-1, 896, 17, 17]         230,272\n",
      "      CustomReLU-228          [-1, 896, 17, 17]               0\n",
      "     Inception_B-229          [-1, 896, 17, 17]               0\n",
      "          Conv2d-230          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-231          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-232          [-1, 128, 17, 17]               0\n",
      "          Conv2d-233          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-234          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-235          [-1, 128, 17, 17]               0\n",
      "          Conv2d-236          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-237          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-238          [-1, 128, 17, 17]               0\n",
      "          Conv2d-239          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-240          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-241          [-1, 128, 17, 17]               0\n",
      "          Conv2d-242          [-1, 896, 17, 17]         230,272\n",
      "      CustomReLU-243          [-1, 896, 17, 17]               0\n",
      "     Inception_B-244          [-1, 896, 17, 17]               0\n",
      "          Conv2d-245          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-246          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-247          [-1, 128, 17, 17]               0\n",
      "          Conv2d-248          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-249          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-250          [-1, 128, 17, 17]               0\n",
      "          Conv2d-251          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-252          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-253          [-1, 128, 17, 17]               0\n",
      "          Conv2d-254          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-255          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-256          [-1, 128, 17, 17]               0\n",
      "          Conv2d-257          [-1, 896, 17, 17]         230,272\n",
      "      CustomReLU-258          [-1, 896, 17, 17]               0\n",
      "     Inception_B-259          [-1, 896, 17, 17]               0\n",
      "          Conv2d-260          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-261          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-262          [-1, 128, 17, 17]               0\n",
      "          Conv2d-263          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-264          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-265          [-1, 128, 17, 17]               0\n",
      "          Conv2d-266          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-267          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-268          [-1, 128, 17, 17]               0\n",
      "          Conv2d-269          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-270          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-271          [-1, 128, 17, 17]               0\n",
      "          Conv2d-272          [-1, 896, 17, 17]         230,272\n",
      "      CustomReLU-273          [-1, 896, 17, 17]               0\n",
      "     Inception_B-274          [-1, 896, 17, 17]               0\n",
      "          Conv2d-275          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-276          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-277          [-1, 128, 17, 17]               0\n",
      "          Conv2d-278          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-279          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-280          [-1, 128, 17, 17]               0\n",
      "          Conv2d-281          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-282          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-283          [-1, 128, 17, 17]               0\n",
      "          Conv2d-284          [-1, 128, 17, 17]         114,816\n",
      "     BatchNorm2d-285          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-286          [-1, 128, 17, 17]               0\n",
      "          Conv2d-287          [-1, 896, 17, 17]         230,272\n",
      "      CustomReLU-288          [-1, 896, 17, 17]               0\n",
      "     Inception_B-289          [-1, 896, 17, 17]               0\n",
      "       MaxPool2d-290            [-1, 896, 8, 8]               0\n",
      "          Conv2d-291          [-1, 256, 17, 17]         229,632\n",
      "     BatchNorm2d-292          [-1, 256, 17, 17]             512\n",
      "      CustomReLU-293          [-1, 256, 17, 17]               0\n",
      "          Conv2d-294            [-1, 384, 8, 8]         885,120\n",
      "     BatchNorm2d-295            [-1, 384, 8, 8]             768\n",
      "      CustomReLU-296            [-1, 384, 8, 8]               0\n",
      "          Conv2d-297          [-1, 256, 17, 17]         229,632\n",
      "     BatchNorm2d-298          [-1, 256, 17, 17]             512\n",
      "      CustomReLU-299          [-1, 256, 17, 17]               0\n",
      "          Conv2d-300            [-1, 256, 8, 8]         590,080\n",
      "     BatchNorm2d-301            [-1, 256, 8, 8]             512\n",
      "      CustomReLU-302            [-1, 256, 8, 8]               0\n",
      "          Conv2d-303          [-1, 256, 17, 17]         229,632\n",
      "     BatchNorm2d-304          [-1, 256, 17, 17]             512\n",
      "      CustomReLU-305          [-1, 256, 17, 17]               0\n",
      "          Conv2d-306          [-1, 256, 17, 17]         590,080\n",
      "     BatchNorm2d-307          [-1, 256, 17, 17]             512\n",
      "      CustomReLU-308          [-1, 256, 17, 17]               0\n",
      "          Conv2d-309            [-1, 256, 8, 8]         590,080\n",
      "     BatchNorm2d-310            [-1, 256, 8, 8]             512\n",
      "      CustomReLU-311            [-1, 256, 8, 8]               0\n",
      "     Reduction_B-312           [-1, 1792, 8, 8]               0\n",
      "          Conv2d-313            [-1, 192, 8, 8]         344,256\n",
      "     BatchNorm2d-314            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-315            [-1, 192, 8, 8]               0\n",
      "          Conv2d-316            [-1, 192, 8, 8]         344,256\n",
      "     BatchNorm2d-317            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-318            [-1, 192, 8, 8]               0\n",
      "          Conv2d-319            [-1, 192, 8, 8]         110,784\n",
      "     BatchNorm2d-320            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-321            [-1, 192, 8, 8]               0\n",
      "          Conv2d-322            [-1, 192, 8, 8]         110,784\n",
      "     BatchNorm2d-323            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-324            [-1, 192, 8, 8]               0\n",
      "          Conv2d-325           [-1, 1792, 8, 8]         689,920\n",
      "      CustomReLU-326           [-1, 1792, 8, 8]               0\n",
      "     Inception_C-327           [-1, 1792, 8, 8]               0\n",
      "          Conv2d-328            [-1, 192, 8, 8]         344,256\n",
      "     BatchNorm2d-329            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-330            [-1, 192, 8, 8]               0\n",
      "          Conv2d-331            [-1, 192, 8, 8]         344,256\n",
      "     BatchNorm2d-332            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-333            [-1, 192, 8, 8]               0\n",
      "          Conv2d-334            [-1, 192, 8, 8]         110,784\n",
      "     BatchNorm2d-335            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-336            [-1, 192, 8, 8]               0\n",
      "          Conv2d-337            [-1, 192, 8, 8]         110,784\n",
      "     BatchNorm2d-338            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-339            [-1, 192, 8, 8]               0\n",
      "          Conv2d-340           [-1, 1792, 8, 8]         689,920\n",
      "      CustomReLU-341           [-1, 1792, 8, 8]               0\n",
      "     Inception_C-342           [-1, 1792, 8, 8]               0\n",
      "          Conv2d-343            [-1, 192, 8, 8]         344,256\n",
      "     BatchNorm2d-344            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-345            [-1, 192, 8, 8]               0\n",
      "          Conv2d-346            [-1, 192, 8, 8]         344,256\n",
      "     BatchNorm2d-347            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-348            [-1, 192, 8, 8]               0\n",
      "          Conv2d-349            [-1, 192, 8, 8]         110,784\n",
      "     BatchNorm2d-350            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-351            [-1, 192, 8, 8]               0\n",
      "          Conv2d-352            [-1, 192, 8, 8]         110,784\n",
      "     BatchNorm2d-353            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-354            [-1, 192, 8, 8]               0\n",
      "          Conv2d-355           [-1, 1792, 8, 8]         689,920\n",
      "      CustomReLU-356           [-1, 1792, 8, 8]               0\n",
      "     Inception_C-357           [-1, 1792, 8, 8]               0\n",
      "          Conv2d-358            [-1, 192, 8, 8]         344,256\n",
      "     BatchNorm2d-359            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-360            [-1, 192, 8, 8]               0\n",
      "          Conv2d-361            [-1, 192, 8, 8]         344,256\n",
      "     BatchNorm2d-362            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-363            [-1, 192, 8, 8]               0\n",
      "          Conv2d-364            [-1, 192, 8, 8]         110,784\n",
      "     BatchNorm2d-365            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-366            [-1, 192, 8, 8]               0\n",
      "          Conv2d-367            [-1, 192, 8, 8]         110,784\n",
      "     BatchNorm2d-368            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-369            [-1, 192, 8, 8]               0\n",
      "          Conv2d-370           [-1, 1792, 8, 8]         689,920\n",
      "      CustomReLU-371           [-1, 1792, 8, 8]               0\n",
      "     Inception_C-372           [-1, 1792, 8, 8]               0\n",
      "          Conv2d-373            [-1, 192, 8, 8]         344,256\n",
      "     BatchNorm2d-374            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-375            [-1, 192, 8, 8]               0\n",
      "          Conv2d-376            [-1, 192, 8, 8]         344,256\n",
      "     BatchNorm2d-377            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-378            [-1, 192, 8, 8]               0\n",
      "          Conv2d-379            [-1, 192, 8, 8]         110,784\n",
      "     BatchNorm2d-380            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-381            [-1, 192, 8, 8]               0\n",
      "          Conv2d-382            [-1, 192, 8, 8]         110,784\n",
      "     BatchNorm2d-383            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-384            [-1, 192, 8, 8]               0\n",
      "          Conv2d-385           [-1, 1792, 8, 8]         689,920\n",
      "      CustomReLU-386           [-1, 1792, 8, 8]               0\n",
      "     Inception_C-387           [-1, 1792, 8, 8]               0\n",
      "       AvgPool2d-388           [-1, 1792, 1, 1]               0\n",
      "         Dropout-389           [-1, 1792, 1, 1]               0\n",
      "          Linear-390                 [-1, 1000]       1,793,000\n",
      "================================================================\n",
      "Total params: 22,769,848\n",
      "Trainable params: 22,769,848\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.02\n",
      "Forward/backward pass size (MB): 309.49\n",
      "Params size (MB): 86.86\n",
      "Estimated Total Size (MB): 397.37\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "model = CustomInceptionResNet_V1()\n",
    "\n",
    "print(model.__class__.__name__)\n",
    "summary(model, input_size=(3, 299, 299))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "afa7d470-8b23-408b-91f6-b1bc4bfd0afc",
   "metadata": {},
   "source": [
    "# Create a dummy input tensor (adjust its shape to match model)\n",
    "dummy_input = torch.randn(1, 3, 299, 299)  # Shape: (batch_size, channels, height, width)\n",
    "    \n",
    "# Initialize the exporter with the model name and Export the model\n",
    "exporter = ModelExporter(model.__class__.__name__)\n",
    "exporter.export_to_onnx(model, dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccf8356-f3f4-4398-a278-0d16bd429081",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
