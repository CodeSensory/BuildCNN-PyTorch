{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\n\nclass CustomReLU(torch.nn.Module):\n    \"\"\"\n    Custom implementation of the ReLU activation function.\n    \"\"\"\n    def __init__(self):\n        super(CustomReLU, self).__init__()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Applies the ReLU function element-wise: max(0, x).\n        Args:\n            x (torch.Tensor): Input tensor\n        Returns:\n            torch.Tensor: Output tensor with ReALU applied\n        \"\"\"\n        return torch.maximum(x, torch.zeros_like(x))","metadata":{"id":"4o4mxVt7sqKd","trusted":true,"execution":{"iopub.status.busy":"2025-01-07T02:18:04.193101Z","iopub.execute_input":"2025-01-07T02:18:04.193494Z","iopub.status.idle":"2025-01-07T02:18:07.485090Z","shell.execute_reply.started":"2025-01-07T02:18:04.193463Z","shell.execute_reply":"2025-01-07T02:18:07.484054Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\n\nclass ConvolutionalLayer:\n    \"\"\"\n    A utility class to create a convolutional layer with a custom ReLU activation.\n    \"\"\"\n    @staticmethod\n    def create(in_channels: int, out_channels: int, kernel_size: int, stride: int, padding: int) -> torch.nn.Sequential:\n        \"\"\"\n        Creates a convolutional layer with a custom ReLU activation.\n        Args:\n            in_channels (int): Number of input channels\n            out_channels (int): Number of output channels\n            kernel_size (int): Size of the convolution kernel\n            stride (int): Stride of the convolution\n            padding (int): Padding added to the input\n        Returns:\n            torch.nn.Sequential: A sequential layer containing Conv2d and CustomReLU\n        \"\"\"\n        return torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n            CustomReLU()\n        )","metadata":{"id":"irmlSrqus6dU","trusted":true,"execution":{"iopub.status.busy":"2025-01-07T02:18:07.486465Z","iopub.execute_input":"2025-01-07T02:18:07.486871Z","iopub.status.idle":"2025-01-07T02:18:07.491865Z","shell.execute_reply.started":"2025-01-07T02:18:07.486836Z","shell.execute_reply":"2025-01-07T02:18:07.490807Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\n\nclass FullyConnectedLayer:\n    \"\"\"\n    A utility class to create Fully Connected (FC) layers with Xavier initialization.\n    \"\"\"\n    @staticmethod\n    def Dense(input_dim: int, output_dim: int) -> torch.nn.Linear:\n        \"\"\"\n        Creates an FC layer and applies Xavier initialization.\n        Args:\n            input_dim (int): Number of input features\n            output_dim (int): Number of output features\n        Returns:\n            torch.nn.Linear: Initialized Fully-Connected layer\n        \"\"\"\n        layer = torch.nn.Linear(input_dim, output_dim, bias=True)\n        torch.nn.init.xavier_uniform_(layer.weight)\n        return layer","metadata":{"id":"A5xoeNALs7kQ","trusted":true,"execution":{"iopub.status.busy":"2025-01-07T02:18:07.493356Z","iopub.execute_input":"2025-01-07T02:18:07.493603Z","iopub.status.idle":"2025-01-07T02:18:07.509232Z","shell.execute_reply.started":"2025-01-07T02:18:07.493582Z","shell.execute_reply":"2025-01-07T02:18:07.508253Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\n\nclass CustomAlexNet(torch.nn.Module):\n    \"\"\"\n    Implementation of the AlexNet model.\n    Input: Image tensor (batch_size, 3, 227, 227)\n    Output: Class scores (batch_size, 1000)\n    \"\"\"\n    def __init__(self, dropout_rate=0.5):\n        \"\"\"\n        Args:\n            dropout_rate (float): Dropout probability (default=0.5)\n        \"\"\"\n        super(CustomAlexNet, self).__init__()\n\n        # Convolutional and pooling layers\n        self.layer1 = torch.nn.Sequential(\n            ConvolutionalLayer.create(3, 96, 11, 4, 0),\n            torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n        )\n\n        self.layer2 = torch.nn.Sequential(\n            ConvolutionalLayer.create(96, 256, 5, 1, 2),\n            torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n        )\n\n        self.layer3 = ConvolutionalLayer.create(256, 384, 3, 1, 1)\n        self.layer4 = ConvolutionalLayer.create(384, 384, 3, 1, 1)\n\n        self.layer5 = torch.nn.Sequential(\n            ConvolutionalLayer.create(384, 256, 3, 1, 1),\n            torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n        )\n\n        # Fully Connected layers and dropout\n        self.layer_drop = torch.nn.Dropout(p=dropout_rate)\n\n        self.layer6 = FullyConnectedLayer.Dense(6 * 6 * 256, 4096)\n\n        self.layer7 = FullyConnectedLayer.Dense(4096, 4096)\n\n        self.layer8 = FullyConnectedLayer.Dense(4096, 1000)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Defines the forward pass of the model.\n        Args:\n            x (torch.Tensor): Input image tensor (batch_size, 3, 227, 227)\n        Returns:\n            torch.Tensor: Class scores (batch_size, 1000)\n        \"\"\"\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n        x = self.layer_drop(x)\n        x = x.view(x.size(0), -1)  # Flatten\n        x = self.layer6(x)\n        x = self.layer7(x)\n        x = self.layer8(x)\n        return x","metadata":{"id":"VT6SNYQVs8wF","trusted":true,"execution":{"iopub.status.busy":"2025-01-07T02:18:07.510460Z","iopub.execute_input":"2025-01-07T02:18:07.510747Z","iopub.status.idle":"2025-01-07T02:18:07.528153Z","shell.execute_reply.started":"2025-01-07T02:18:07.510725Z","shell.execute_reply":"2025-01-07T02:18:07.527248Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchsummary import summary\n\n# Check device compatibility (GPU or CPU)\ndef get_device():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    device_name = \"GPU\" if torch.cuda.is_available() else \"CPU\"\n    print(f\"Using {device_name} for computation.\")\n    return device\n\n# CIFAR-10 Dataset Load\ndef get_dataloader(batch_size=64):\n    transform = transforms.Compose([\n        transforms.Resize((227, 227)),  # AlexNet의 입력 크기 227x227\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Imagenet 평균과 표준편차\n    ])\n\n    # Training DataLoader\n    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n\n    # Test DataLoader\n    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n\n    return trainloader, testloader\n\n# Print Model Summary\ndef print_model_summary(model, device):\n    summary(model.to(device), input_size=(3, 227, 227))\n\n# Model Trainning\ndef train(model, trainloader, criterion, optimizer, device, epochs=1, testloader=None):\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n\n        for inputs, labels in trainloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        train_loss = running_loss / len(trainloader)\n        train_accuracy = correct / total * 100\n\n        # If testloader is provided, calculate test loss\n        if testloader:\n            model.eval()\n            test_running_loss = 0.0\n            with torch.no_grad():\n                for inputs, labels in testloader:\n                    inputs, labels = inputs.to(device), labels.to(device)\n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels)\n                    test_running_loss += loss.item()\n            test_loss = test_running_loss / len(testloader)\n        else:\n            test_loss = \"N/A\"  # Default value if no testloader is provided\n\n        # Print metrics\n        print(f\"Epoch [{epoch + 1}/{epochs}] - \"\n              f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, \"\n              f\"Test Loss: {test_loss}\")\n\n# Model Test\ndef test(model, testloader, device):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in testloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    accuracy = correct / total * 100\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n\n# Trainning & Evaluation\ndef main(epochs=10):\n    # Set device\n    device = get_device()\n\n    trainloader, testloader = get_dataloader()\n\n    model = CustomAlexNet()\n    print_model_summary(model, device)\n\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    train(model, trainloader, criterion, optimizer, device, epochs)\n    test(model, testloader, device)\n\nif __name__ == \"__main__\":\n    epochs = 100  # Set the number of epochs for training\n    main(epochs)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q8Upz_8_tog2","outputId":"b56e413a-6bc1-4364-9557-3efe72df9dc3","trusted":true,"execution":{"iopub.status.busy":"2025-01-07T02:18:07.529055Z","iopub.execute_input":"2025-01-07T02:18:07.529289Z","iopub.status.idle":"2025-01-07T05:53:26.846884Z","shell.execute_reply.started":"2025-01-07T02:18:07.529268Z","shell.execute_reply":"2025-01-07T05:53:26.845824Z"}},"outputs":[{"name":"stdout","text":"Using GPU for computation.\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:02<00:00, 76839469.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 96, 55, 55]          34,944\n        CustomReLU-2           [-1, 96, 55, 55]               0\n         MaxPool2d-3           [-1, 96, 27, 27]               0\n            Conv2d-4          [-1, 256, 27, 27]         614,656\n        CustomReLU-5          [-1, 256, 27, 27]               0\n         MaxPool2d-6          [-1, 256, 13, 13]               0\n            Conv2d-7          [-1, 384, 13, 13]         885,120\n        CustomReLU-8          [-1, 384, 13, 13]               0\n            Conv2d-9          [-1, 384, 13, 13]       1,327,488\n       CustomReLU-10          [-1, 384, 13, 13]               0\n           Conv2d-11          [-1, 256, 13, 13]         884,992\n       CustomReLU-12          [-1, 256, 13, 13]               0\n        MaxPool2d-13            [-1, 256, 6, 6]               0\n          Dropout-14            [-1, 256, 6, 6]               0\n           Linear-15                 [-1, 4096]      37,752,832\n           Linear-16                 [-1, 4096]      16,781,312\n           Linear-17                 [-1, 1000]       4,097,000\n================================================================\nTotal params: 62,378,344\nTrainable params: 62,378,344\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.59\nForward/backward pass size (MB): 10.99\nParams size (MB): 237.95\nEstimated Total Size (MB): 249.54\n----------------------------------------------------------------\nEpoch [1/100] - Train Loss: 2.1459, Train Accuracy: 25.81%, Test Loss: N/A\nEpoch [2/100] - Train Loss: 1.6632, Train Accuracy: 38.65%, Test Loss: N/A\nEpoch [3/100] - Train Loss: 1.5384, Train Accuracy: 44.02%, Test Loss: N/A\nEpoch [4/100] - Train Loss: 1.4833, Train Accuracy: 46.10%, Test Loss: N/A\nEpoch [5/100] - Train Loss: 1.4543, Train Accuracy: 47.24%, Test Loss: N/A\nEpoch [6/100] - Train Loss: 1.4251, Train Accuracy: 48.60%, Test Loss: N/A\nEpoch [7/100] - Train Loss: 1.4064, Train Accuracy: 49.27%, Test Loss: N/A\nEpoch [8/100] - Train Loss: 1.4014, Train Accuracy: 49.58%, Test Loss: N/A\nEpoch [9/100] - Train Loss: 1.3986, Train Accuracy: 49.43%, Test Loss: N/A\nEpoch [10/100] - Train Loss: 1.4167, Train Accuracy: 49.16%, Test Loss: N/A\nEpoch [11/100] - Train Loss: 1.3884, Train Accuracy: 50.21%, Test Loss: N/A\nEpoch [12/100] - Train Loss: 1.3994, Train Accuracy: 49.66%, Test Loss: N/A\nEpoch [13/100] - Train Loss: 1.3824, Train Accuracy: 50.33%, Test Loss: N/A\nEpoch [14/100] - Train Loss: 1.3944, Train Accuracy: 50.16%, Test Loss: N/A\nEpoch [15/100] - Train Loss: 1.3812, Train Accuracy: 50.70%, Test Loss: N/A\nEpoch [16/100] - Train Loss: 1.3763, Train Accuracy: 50.37%, Test Loss: N/A\nEpoch [17/100] - Train Loss: 1.3627, Train Accuracy: 51.10%, Test Loss: N/A\nEpoch [18/100] - Train Loss: 1.3659, Train Accuracy: 51.03%, Test Loss: N/A\nEpoch [19/100] - Train Loss: 1.3775, Train Accuracy: 50.66%, Test Loss: N/A\nEpoch [20/100] - Train Loss: 1.3657, Train Accuracy: 50.86%, Test Loss: N/A\nEpoch [21/100] - Train Loss: 1.3811, Train Accuracy: 50.37%, Test Loss: N/A\nEpoch [22/100] - Train Loss: 1.3796, Train Accuracy: 50.39%, Test Loss: N/A\nEpoch [23/100] - Train Loss: 1.3860, Train Accuracy: 50.38%, Test Loss: N/A\nEpoch [24/100] - Train Loss: 1.3740, Train Accuracy: 50.87%, Test Loss: N/A\nEpoch [25/100] - Train Loss: 1.3597, Train Accuracy: 51.21%, Test Loss: N/A\nEpoch [26/100] - Train Loss: 1.3594, Train Accuracy: 51.14%, Test Loss: N/A\nEpoch [27/100] - Train Loss: 1.3514, Train Accuracy: 51.28%, Test Loss: N/A\nEpoch [28/100] - Train Loss: 1.3670, Train Accuracy: 51.13%, Test Loss: N/A\nEpoch [29/100] - Train Loss: 1.3423, Train Accuracy: 52.15%, Test Loss: N/A\nEpoch [30/100] - Train Loss: 1.3522, Train Accuracy: 51.73%, Test Loss: N/A\nEpoch [31/100] - Train Loss: 1.3514, Train Accuracy: 51.72%, Test Loss: N/A\nEpoch [32/100] - Train Loss: 1.3475, Train Accuracy: 51.75%, Test Loss: N/A\nEpoch [33/100] - Train Loss: 1.3508, Train Accuracy: 51.79%, Test Loss: N/A\nEpoch [34/100] - Train Loss: 1.3386, Train Accuracy: 52.24%, Test Loss: N/A\nEpoch [35/100] - Train Loss: 1.3506, Train Accuracy: 51.81%, Test Loss: N/A\nEpoch [36/100] - Train Loss: 1.3591, Train Accuracy: 51.35%, Test Loss: N/A\nEpoch [37/100] - Train Loss: 1.3364, Train Accuracy: 52.32%, Test Loss: N/A\nEpoch [38/100] - Train Loss: 1.3396, Train Accuracy: 52.41%, Test Loss: N/A\nEpoch [39/100] - Train Loss: 1.3478, Train Accuracy: 51.88%, Test Loss: N/A\nEpoch [40/100] - Train Loss: 1.3484, Train Accuracy: 52.07%, Test Loss: N/A\nEpoch [41/100] - Train Loss: 1.3831, Train Accuracy: 50.66%, Test Loss: N/A\nEpoch [42/100] - Train Loss: 1.3492, Train Accuracy: 51.80%, Test Loss: N/A\nEpoch [43/100] - Train Loss: 1.3395, Train Accuracy: 51.97%, Test Loss: N/A\nEpoch [44/100] - Train Loss: 1.3494, Train Accuracy: 51.80%, Test Loss: N/A\nEpoch [45/100] - Train Loss: 1.3692, Train Accuracy: 50.82%, Test Loss: N/A\nEpoch [46/100] - Train Loss: 1.3782, Train Accuracy: 50.61%, Test Loss: N/A\nEpoch [47/100] - Train Loss: 1.3990, Train Accuracy: 50.05%, Test Loss: N/A\nEpoch [48/100] - Train Loss: 1.3569, Train Accuracy: 51.13%, Test Loss: N/A\nEpoch [49/100] - Train Loss: 1.3455, Train Accuracy: 51.76%, Test Loss: N/A\nEpoch [50/100] - Train Loss: 1.3796, Train Accuracy: 50.84%, Test Loss: N/A\nEpoch [51/100] - Train Loss: 1.3550, Train Accuracy: 51.38%, Test Loss: N/A\nEpoch [52/100] - Train Loss: 1.3404, Train Accuracy: 52.11%, Test Loss: N/A\nEpoch [53/100] - Train Loss: 1.3441, Train Accuracy: 51.72%, Test Loss: N/A\nEpoch [54/100] - Train Loss: 1.3508, Train Accuracy: 51.66%, Test Loss: N/A\nEpoch [55/100] - Train Loss: 1.3524, Train Accuracy: 51.85%, Test Loss: N/A\nEpoch [56/100] - Train Loss: 1.3382, Train Accuracy: 52.15%, Test Loss: N/A\nEpoch [57/100] - Train Loss: 1.3394, Train Accuracy: 51.70%, Test Loss: N/A\nEpoch [58/100] - Train Loss: 1.3599, Train Accuracy: 51.38%, Test Loss: N/A\nEpoch [59/100] - Train Loss: 1.3425, Train Accuracy: 52.12%, Test Loss: N/A\nEpoch [60/100] - Train Loss: 1.3353, Train Accuracy: 52.07%, Test Loss: N/A\nEpoch [61/100] - Train Loss: 1.3536, Train Accuracy: 51.85%, Test Loss: N/A\nEpoch [62/100] - Train Loss: 1.3318, Train Accuracy: 52.47%, Test Loss: N/A\nEpoch [63/100] - Train Loss: 1.3465, Train Accuracy: 51.83%, Test Loss: N/A\nEpoch [64/100] - Train Loss: 1.3248, Train Accuracy: 52.53%, Test Loss: N/A\nEpoch [65/100] - Train Loss: 1.3723, Train Accuracy: 51.04%, Test Loss: N/A\nEpoch [66/100] - Train Loss: 1.5870, Train Accuracy: 43.43%, Test Loss: N/A\nEpoch [67/100] - Train Loss: 2.3100, Train Accuracy: 10.08%, Test Loss: N/A\nEpoch [68/100] - Train Loss: 2.2378, Train Accuracy: 14.96%, Test Loss: N/A\nEpoch [69/100] - Train Loss: 2.0774, Train Accuracy: 23.10%, Test Loss: N/A\nEpoch [70/100] - Train Loss: 1.8092, Train Accuracy: 33.31%, Test Loss: N/A\nEpoch [71/100] - Train Loss: 1.6959, Train Accuracy: 37.62%, Test Loss: N/A\nEpoch [72/100] - Train Loss: 1.6279, Train Accuracy: 40.32%, Test Loss: N/A\nEpoch [73/100] - Train Loss: 1.5678, Train Accuracy: 42.87%, Test Loss: N/A\nEpoch [74/100] - Train Loss: 1.5238, Train Accuracy: 44.54%, Test Loss: N/A\nEpoch [75/100] - Train Loss: 1.5033, Train Accuracy: 45.10%, Test Loss: N/A\nEpoch [76/100] - Train Loss: 1.4757, Train Accuracy: 46.46%, Test Loss: N/A\nEpoch [77/100] - Train Loss: 1.4679, Train Accuracy: 46.94%, Test Loss: N/A\nEpoch [78/100] - Train Loss: 1.4654, Train Accuracy: 47.33%, Test Loss: N/A\nEpoch [79/100] - Train Loss: 1.4496, Train Accuracy: 47.79%, Test Loss: N/A\nEpoch [80/100] - Train Loss: 1.4687, Train Accuracy: 46.84%, Test Loss: N/A\nEpoch [81/100] - Train Loss: 1.4542, Train Accuracy: 47.47%, Test Loss: N/A\nEpoch [82/100] - Train Loss: 1.4560, Train Accuracy: 47.46%, Test Loss: N/A\nEpoch [83/100] - Train Loss: 1.4564, Train Accuracy: 47.68%, Test Loss: N/A\nEpoch [84/100] - Train Loss: 1.4392, Train Accuracy: 47.76%, Test Loss: N/A\nEpoch [85/100] - Train Loss: 1.4121, Train Accuracy: 48.82%, Test Loss: N/A\nEpoch [86/100] - Train Loss: 1.4308, Train Accuracy: 48.24%, Test Loss: N/A\nEpoch [87/100] - Train Loss: 1.4327, Train Accuracy: 48.26%, Test Loss: N/A\nEpoch [88/100] - Train Loss: 1.4119, Train Accuracy: 49.07%, Test Loss: N/A\nEpoch [89/100] - Train Loss: 1.4160, Train Accuracy: 48.96%, Test Loss: N/A\nEpoch [90/100] - Train Loss: 1.4212, Train Accuracy: 48.75%, Test Loss: N/A\nEpoch [91/100] - Train Loss: 1.3999, Train Accuracy: 49.53%, Test Loss: N/A\nEpoch [92/100] - Train Loss: 1.4193, Train Accuracy: 49.11%, Test Loss: N/A\nEpoch [93/100] - Train Loss: 1.4227, Train Accuracy: 48.65%, Test Loss: N/A\nEpoch [94/100] - Train Loss: 1.3993, Train Accuracy: 49.35%, Test Loss: N/A\nEpoch [95/100] - Train Loss: 1.3916, Train Accuracy: 49.77%, Test Loss: N/A\nEpoch [96/100] - Train Loss: 1.4135, Train Accuracy: 49.14%, Test Loss: N/A\nEpoch [97/100] - Train Loss: 1.4031, Train Accuracy: 49.31%, Test Loss: N/A\nEpoch [98/100] - Train Loss: 1.4386, Train Accuracy: 48.52%, Test Loss: N/A\nEpoch [99/100] - Train Loss: 1.4285, Train Accuracy: 48.63%, Test Loss: N/A\nEpoch [100/100] - Train Loss: 1.4055, Train Accuracy: 49.20%, Test Loss: N/A\nTest Accuracy: 51.35%\n","output_type":"stream"}],"execution_count":5}]}