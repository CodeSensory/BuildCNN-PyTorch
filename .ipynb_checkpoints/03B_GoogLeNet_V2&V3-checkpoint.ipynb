{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be5e53d0-3940-40a8-9ec8-2c6b62ebe382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CustomReLU(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Custom implementation of the ReLU activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CustomReLU, self).__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Applies the ReLU function element-wise: max(0, x).\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor with ReALU applied\n",
    "        \"\"\"\n",
    "        return torch.maximum(x, torch.zeros_like(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5eea940-b550-4e63-b329-30c7cda5c6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ConvolutionalLayer:\n",
    "    \"\"\"\n",
    "    A utility class to create a convolutional layer with a custom ReLU activation.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def create(in_channels: int, out_channels: int, kernel_size: int, stride: int, padding: int) -> torch.nn.Sequential:\n",
    "        \"\"\"\n",
    "        Creates a convolutional layer with a custom ReLU activation.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels\n",
    "            out_channels (int): Number of output channels\n",
    "            kernel_size (int): Size of the convolution kernel\n",
    "            stride (int): Stride of the convolution\n",
    "            padding (int): Padding added to the input\n",
    "        Returns:\n",
    "            torch.nn.Sequential: A sequential layer containing Conv2d and CustomReLU(+Batch Normalization)\n",
    "        \"\"\"\n",
    "        return torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            CustomReLU()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a26622b-4486-445e-a538-271aa91d0e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class FullyConnectedLayer:\n",
    "    \"\"\"\n",
    "    A utility class to create Fully Connected (FC) layers with Xavier initialization.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def Dense(input_dim: int, output_dim: int) -> torch.nn.Linear:\n",
    "        \"\"\"\n",
    "        Creates an FC layer and applies Xavier initialization.\n",
    "        Args:\n",
    "            input_dim (int): Number of input features\n",
    "            output_dim (int): Number of output features\n",
    "        Returns:\n",
    "            torch.nn.Linear: Initialized Fully-Connected layer\n",
    "        \"\"\"\n",
    "        layer = torch.nn.Linear(input_dim, output_dim, bias=True)\n",
    "        torch.nn.init.xavier_uniform_(layer.weight)\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1f6a434-18f0-4cf1-9b1d-94e160bebd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Inception_A(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the Inception A layer(Base on Figure 5) introduced by GoogleNet_V2 and V3\n",
    "\n",
    "    Branch 1: 1x1 Convolution\n",
    "    Branch 2: 1×1 → 3×3 Convolution\n",
    "    Branch 3: 1×1 → 3×3 → 3×3 Convolution\n",
    "    Branch 4: Average Pooling → 1×1 Convolution\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_branch1: int, reduce_branch2: int, out_branch2: int, \n",
    "                 reduce_branch3: int, out_branch3: int, out_branch4: int) -> torch.nn.Sequential:\n",
    "        \"\"\"\n",
    "        Creates a Inception layer with a custom ReLU activation Function.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels\n",
    "            out_branch1 (int): Number of output channels for Branch 1\n",
    "            reduce_branch2 (int): Number of output channels for 3x3 convolution reduction\n",
    "            out_branch2 (int): Number of output channels for Branch 2\n",
    "            reduce_branch3 (int): Number of output channels for 3x3 convolution reduction\n",
    "            out_branch3 (int): Number of output channels for Branch 3\n",
    "            out_branch4 (int): Number of output channels for Branch 4\n",
    "        Returns:\n",
    "            torch.nn.Sequential: A sequential block representing the Inception_A layer\n",
    "        \"\"\"\n",
    "        super(Inception_A, self).__init__()\n",
    "\n",
    "        self.branch1 = ConvolutionalLayer.create(in_channels, out_branch1, 1, 1, 0)\n",
    "        \n",
    "        self.branch2 = torch.nn.Sequential(\n",
    "            ConvolutionalLayer.create(in_channels, reduce_branch2, 3, 1, 1),\n",
    "            ConvolutionalLayer.create(reduce_branch2, out_branch2, 3, 1, 1)\n",
    "        )\n",
    "\n",
    "        self.branch3 = torch.nn.Sequential(\n",
    "            ConvolutionalLayer.create(in_channels, reduce_branch3, 3, 1, 1),\n",
    "            ConvolutionalLayer.create(reduce_branch3, out_branch3, 3, 1, 1),\n",
    "            ConvolutionalLayer.create(out_branch3, out_branch3, 3, 1, 1)\n",
    "        )\n",
    "\n",
    "        self.branch4 = torch.nn.Sequential(\n",
    "            torch.nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            ConvolutionalLayer.create(in_channels, out_branch4, 1, 1, 0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the InceptionV1 layer.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after passing through all branches of the Inception_A layer\n",
    "        \"\"\"\n",
    "        return torch.cat([self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fcfd0e8-0843-4828-a620-83dd31abf196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Inception_B(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the Inception B layer(Base on Figure 6) introduced by GoogleNet_V2 and V3\n",
    "\n",
    "    Branch 1: 1x1 Convolution\n",
    "    Branch 2: 1×1 → 1×7 → 7×1 Convolution\n",
    "    Branch 3: 1×1 → 7×1 → 1×7 → 7×1 → 1×7 Convolution\n",
    "    Branch 4: Max Pooling → 1×1 Convolution\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> torch.nn.Sequential:\n",
    "        \"\"\"\n",
    "        Creates a Inception layer with a custom ReLU activation Function.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels\n",
    "            out_channels (int): Number of output channels\n",
    "        Returns:\n",
    "            torch.nn.Sequential: A sequential block representing the Inception_B layer\n",
    "        \"\"\"\n",
    "        super(Inception_B, self).__init__()\n",
    "\n",
    "        self.branch1 = ConvolutionalLayer.create(in_channels, 192, 1, 1, 0)  # Output value is fixed to 192\n",
    "        \n",
    "        self.branch2 = torch.nn.Sequential(\n",
    "            ConvolutionalLayer.create(in_channels, 192, 1, 1, 0),\n",
    "            ConvolutionalLayer.create(192, 384, (1, 7), 1, (0, 3)),\n",
    "            ConvolutionalLayer.create(384, 384, (7, 1), 1, (3, 0))\n",
    "        )\n",
    "\n",
    "        self.branch3 = torch.nn.Sequential(\n",
    "            ConvolutionalLayer.create(in_channels, 192, 1, 1, 0),\n",
    "            ConvolutionalLayer.create(192, 384, (1, 7), 1, (0, 3)),\n",
    "            ConvolutionalLayer.create(384, 384, (7, 1), 1, (3, 0)),\n",
    "            ConvolutionalLayer.create(384, 384, (1, 7), 1, (0, 3)),\n",
    "            ConvolutionalLayer.create(384, 384, (7, 1), 1, (3, 0))\n",
    "        )\n",
    "\n",
    "        self.branch4 = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            ConvolutionalLayer.create(in_channels, 320, 1, 1, 0)    # Output value is fixed to 320\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the InceptionV1 layer.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after passing through all branches of the Inception_A layer\n",
    "        \"\"\"\n",
    "        return torch.cat([self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82847a08-bf8e-4b69-8ddb-368c35aa1a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Inception_C(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the Inception C layer(Base on Figure 7) introduced by GoogleNet_V2 and V3\n",
    "\n",
    "    Branch 1: 1x1 Convolution\n",
    "    Branch 2: 1×1 → (1×3 + 3×1) Convolution\n",
    "    Branch 3: 1×1 → 3×3 → (1×3 + 3×1) Convolution\n",
    "    Branch 4: Average Pooling → 1×1 Convolution\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_branch1: int, reduce_branch2: int, \n",
    "                 reduce_branch3: int, out_branch4: int) -> torch.nn.Sequential:\n",
    "        \"\"\"\n",
    "        Creates a Inception layer with a custom ReLU activation Function.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels\n",
    "            out_branch1 (int): Number of output channels for Branch 1\n",
    "            reduce_branch2 (int): Number of output channels for 3x3 convolution reduction\n",
    "            reduce_branch3 (int): Number of output channels for 3x3 convolution reduction\n",
    "            out_branch4 (int): Number of output channels for Branch 4\n",
    "        Returns:\n",
    "            torch.nn.Sequential: A sequential block representing the Inception_A layer\n",
    "        \"\"\"\n",
    "        super(Inception_C, self).__init__()\n",
    "\n",
    "        self.branch1 = ConvolutionalLayer.create(in_channels, out_branch1, 1, 1, 0)\n",
    "        \n",
    "        self.branch2_a = ConvolutionalLayer.create(in_channels, 384, 1, 1, 0)\n",
    "        self.branch2_b1 = ConvolutionalLayer.create(384, reduce_branch2, (1, 3), 1, (0, 1))\n",
    "        self.branch2_b2 = ConvolutionalLayer.create(384, reduce_branch2, (3, 1), 1, (1, 0))\n",
    "\n",
    "        self.branch3_a = ConvolutionalLayer.create(in_channels, 384, 1, 1, 0)\n",
    "        self.branch3_b = ConvolutionalLayer.create(384, 448, 1, 1, 0)\n",
    "        self.branch2_c1 = ConvolutionalLayer.create(448, reduce_branch3, (1, 3), 1, (0, 1))\n",
    "        self.branch2_c2 = ConvolutionalLayer.create(448, reduce_branch3, (3, 1), 1, (1, 0))\n",
    "\n",
    "        self.branch4 = torch.nn.Sequential(\n",
    "            torch.nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            ConvolutionalLayer.create(in_channels, out_branch4, 1, 1, 0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the InceptionV1 layer.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after passing through all branches of the Inception_A layer\n",
    "        \"\"\"\n",
    "        x_b1 = self.branch1(x)\n",
    "\n",
    "        x_b2_a = self.branch2_a(x)\n",
    "        x_b2_b1 = self.branch2_b1(x_b2_a)\n",
    "        x_b2_b2 = self.branch2_b2(x_b2_a)\n",
    "        x_b2_cat = torch.cat([x_b2_b1, x_b2_b2], dim=1)\n",
    "\n",
    "        x_b3_a = self.branch3_a(x)\n",
    "        x_b3_b = self.branch3_b(x_b3_a)\n",
    "        x_b3_c1 = self.branch3_c1(x_b3_b)\n",
    "        x_b3_c2 = self.branch3_c2(x_b3_b)\n",
    "        x_b3_cat = torch.cat([x_b3_c1, x_b3_c2], dim=1)\n",
    "\n",
    "        x_b4 = self.branch4(x)\n",
    "\n",
    "        return torch.cat([x_b1, x_b2_cat, x_b3_cat, x_b4], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8d7abf1-d37b-4abf-8b56-e81be5c4ce27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CustomGoogLeNet_V3(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GoogLeNet_V2 and V3 model.\n",
    "    Input: Image tensor (batch_size, 3, 299, 299)\n",
    "    Output: Class scores (batch_size, 1000)\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_rate=0.4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dropout_rate (float): Dropout Rate = 0.4(Base on Paper)\n",
    "        \"\"\"\n",
    "        super(CustomGoogLeNet_V3, self).__init__()\n",
    "\n",
    "        # Convolutional and pooling layers\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            ConvolutionalLayer.create(3, 32, 3, 2, 0),\n",
    "            ConvolutionalLayer.create(32, 32, 3, 1, 0),\n",
    "            ConvolutionalLayer.create(32, 64, 3, 1, 1),\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "        )\n",
    "\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            ConvolutionalLayer.create(64, 80, 3, 1, 0),\n",
    "            ConvolutionalLayer.create(80, 192, 3, 2, 0),\n",
    "            ConvolutionalLayer.create(192, 288, 3, 1, 1),\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            Inception_A(192, 64, 48, 64, 64, 96, 32),\n",
    "            Inception_A(256, 64, 48, 64, 64, 96, 64),\n",
    "            Inception_A(288, 128, 128, 256, 128, 256, 128),\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "        )\n",
    "\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            Inception_B(768, 1280),\n",
    "            Inception_B(1280, 1280),\n",
    "            Inception_B(1280, 1280),\n",
    "            Inception_B(1280, 1280),\n",
    "            Inception_B(1280, 1280),\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "        )\n",
    "\n",
    "        self.layer5 = torch.nn.Sequential(\n",
    "            Inception_C(1280, 256, 192, 192, 128),\n",
    "            Inception_C(1152, 384, 352, 352, 256),\n",
    "            torch.nn.AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "        # Fully Connected layers and dropout\n",
    "        self.layer_drop = torch.nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        self.layer6 = FullyConnectedLayer.Dense(1 * 1 * 2048, 1000)\n",
    "\n",
    "        self.layer7 = FullyConnectedLayer.Dense(1000, 1000)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input image tensor (batch_size, 3, 224, 224)\n",
    "        Returns:\n",
    "            torch.Tensor: Class scores (batch_size, 1000)\n",
    "        \"\"\"\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer_drop(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.layer6(x)\n",
    "        x = self.layer7(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b706fc53-973b-4e8f-a943-a097da5099f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 192, 1, 1], expected input[2, 288, 35, 35] to have 192 channels, but got 288 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchsummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m CustomGoogLeNet_V3()\n\u001b[1;32m----> 4\u001b[0m summary(model, input_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m299\u001b[39m, \u001b[38;5;241m299\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\buildPytorch\\Lib\\site-packages\\torchsummary\\torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m model(\u001b[38;5;241m*\u001b[39mx)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\buildPytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\buildPytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[7], line 70\u001b[0m, in \u001b[0;36mCustomGoogLeNet_V3.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     68\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[0;32m     69\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[1;32m---> 70\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n\u001b[0;32m     71\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n\u001b[0;32m     72\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer5(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\buildPytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\buildPytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\buildPytorch\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\buildPytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\buildPytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1844\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m   1843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1846\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[0;32m   1847\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[0;32m   1848\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[0;32m   1849\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\buildPytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1790\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1787\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1788\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1790\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1792\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   1793\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1794\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1795\u001b[0m     ):\n\u001b[0;32m   1796\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 56\u001b[0m, in \u001b[0;36mInception_A.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     48\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Defines the forward pass of the InceptionV1 layer.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m        torch.Tensor: Output tensor after passing through all branches of the Inception_A layer\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch1(x), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch2(x), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch3(x), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch4(x)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\buildPytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\buildPytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\buildPytorch\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\buildPytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\buildPytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1844\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m   1843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1846\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[0;32m   1847\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[0;32m   1848\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[0;32m   1849\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\buildPytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1790\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1787\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1788\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1790\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1792\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   1793\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1794\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1795\u001b[0m     ):\n\u001b[0;32m   1796\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\buildPytorch\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\buildPytorch\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[0;32m    551\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 192, 1, 1], expected input[2, 288, 35, 35] to have 192 channels, but got 288 channels instead"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "model = CustomGoogLeNet_V3()\n",
    "summary(model, input_size=(3, 299, 299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3650956b-ac0c-4bbe-a053-0d585765090c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
