{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9dce5f7-6714-451a-a095-e1a5a0871306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ModelExporter:\n",
    "    \"\"\"\n",
    "    A utility class to export a PyTorch model to an ONNX file.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name):\n",
    "        \"\"\"\n",
    "        Initialize the exporter with a model name.\n",
    "        Args:\n",
    "            model_name (str): The name of the model. This will be used as the ONNX file name.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def export_to_onnx(self, model, dummy_input):\n",
    "        \"\"\"\n",
    "        Exports the given PyTorch model to an ONNX file.\n",
    "        Args:\n",
    "            model (torch.nn.Module): The PyTorch model to be exported.\n",
    "            dummy_input (torch.Tensor): A dummy input tensor that matches the input shape of the model.\n",
    "        \"\"\"\n",
    "        # Generate the ONNX file name using the model name\n",
    "        onnx_file_name = f\"model_onnx/{self.model_name}.onnx\"\n",
    "\n",
    "        # Export the model to ONNX format\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            onnx_file_name,           # Name of the output ONNX file\n",
    "            export_params=True,       # Include the trained parameters in the exported file\n",
    "            opset_version=11,         # Specify the ONNX opset version\n",
    "            do_constant_folding=True, # Perform constant folding optimization\n",
    "            input_names=[\"input\"],    # Name of the input tensor\n",
    "            output_names=[\"output\"]   # Name of the output tensor\n",
    "        )\n",
    "        print(f\"The ONNX file has been saved as '{onnx_file_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a141462-e8bc-4727-8a7f-2df88e1dba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CustomReLU(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Custom implementation of the ReLU activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CustomReLU, self).__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Applies the ReLU function element-wise: max(0, x).\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor with ReLU applied\n",
    "        \"\"\"\n",
    "        return torch.maximum(x, torch.zeros_like(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73ef2b6b-0f07-4601-a602-44174a8eb473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ConvolutionalLayer:\n",
    "    \"\"\"\n",
    "    A utility class to create a convolutional layer with a custom ReLU activation.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def create(in_channels: int, out_channels: int, kernel_size: int, stride: int, padding: int) -> torch.nn.Sequential:\n",
    "        \"\"\"\n",
    "        Creates a convolutional layer with a custom ReLU activation.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels\n",
    "            out_channels (int): Number of output channels\n",
    "            kernel_size (int): Size of the convolution kernel\n",
    "            stride (int): Stride of the convolution\n",
    "            padding (int): Padding added to the input\n",
    "        Returns:\n",
    "            torch.nn.Sequential: A sequential layer containing Conv2d and CustomReLU(+Batch Normalization)\n",
    "        \"\"\"\n",
    "        return torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            CustomReLU()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2efcacde-ae2d-4e7d-bc5c-f35a806c0843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class FullyConnectedLayer:\n",
    "    \"\"\"\n",
    "    A utility class to create Fully Connected (FC) layers with Xavier initialization.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def Dense(input_dim: int, output_dim: int) -> torch.nn.Linear:\n",
    "        \"\"\"\n",
    "        Creates an FC layer and applies Xavier initialization.\n",
    "        Args:\n",
    "            input_dim (int): Number of input features\n",
    "            output_dim (int): Number of output features\n",
    "        Returns:\n",
    "            torch.nn.Linear: Initialized Fully-Connected layer\n",
    "        \"\"\"\n",
    "        layer = torch.nn.Linear(input_dim, output_dim, bias=True)\n",
    "        torch.nn.init.xavier_uniform_(layer.weight)\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "165f11a9-0b07-410b-92d9-38795e8d27f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Stem(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the Stem layer(Base on Figure 13) introduced by Inception V4 and Inception-ResNet-V2\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> torch.nn.Sequential:\n",
    "        \"\"\"\n",
    "        Creates a Inception layer with a custom ReLU activation Function.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels\n",
    "            out_channels (int): Number of output channels\n",
    "        Returns:\n",
    "            torch.nn.Sequential: A sequential block representing the Stem Layer\n",
    "        \"\"\"\n",
    "        super(Stem, self).__init__()\n",
    "\n",
    "        self.step1 = ConvolutionalLayer.create(in_channels, 32, 3, 2, 0)\n",
    "\n",
    "        self.step2 = ConvolutionalLayer.create(32, 32, 3, 1, 0)\n",
    "\n",
    "        self.step3 = ConvolutionalLayer.create(32, 64, 3, 1, 1)\n",
    "\n",
    "        self.step4_branch_a1 = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "        self.step4_branch_b1 = ConvolutionalLayer.create(64, 96, 3, 2, 0)\n",
    "\n",
    "        self.step5_branch_a1 = ConvolutionalLayer.create(160, 64, 1, 1, 0)\n",
    "        self.step5_branch_a2 = ConvolutionalLayer.create(64, 96, 3, 1, 0)\n",
    "        self.step5_branch_b1 = ConvolutionalLayer.create(160, 64, 1, 1, 0)\n",
    "        self.step5_branch_b2 = ConvolutionalLayer.create(64, 64, (1, 7), 1, (0, 3))\n",
    "        self.step5_branch_b3 = ConvolutionalLayer.create(64, 64, (7, 1), 1, (3, 0))\n",
    "        self.step5_branch_b4 = ConvolutionalLayer.create(64, 96, 3, 1, 0)\n",
    "\n",
    "        self.step6_branch_a1 = ConvolutionalLayer.create(192, out_channels - 192, 3, 2, 0)  # In Paper, (192,192,3,2,0)\n",
    "        self.step6_branch_b1 = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the Inception-ResNet-V2 layer.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width)\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after passing through all branches of the Stem layer\n",
    "        \"\"\"\n",
    "        x = self.step1(x)\n",
    "        x = self.step2(x)\n",
    "        x = self.step3(x)\n",
    "\n",
    "        x_b1 = self.step4_branch_a1(x)\n",
    "        x_b2 = self.step4_branch_b1(x)\n",
    "        x = torch.cat([x_b1, x_b2], dim=1)\n",
    "\n",
    "        x_b1 = self.step5_branch_a1(x)\n",
    "        x_b1 = self.step5_branch_a2(x_b1)\n",
    "        x_b2 = self.step5_branch_b1(x)\n",
    "        x_b2 = self.step5_branch_b2(x_b2)\n",
    "        x_b2 = self.step5_branch_b3(x_b2)\n",
    "        x_b2 = self.step5_branch_b4(x_b2)\n",
    "        x = torch.cat([x_b1, x_b2], dim=1)\n",
    "\n",
    "        x_b1 = self.step6_branch_a1(x)\n",
    "        x_b2 = self.step6_branch_b1(x)\n",
    "        x = torch.cat([x_b1, x_b2], dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebe6c5db-b839-46a5-8caa-a081eeb90972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Inception_A(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the Inception_A layer(Base on Figure 16) introduced by Inception-ResNet-V2\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> torch.nn.Sequential:\n",
    "        \"\"\"\n",
    "        Creates a Inception layer with a custom ReLU activation Function.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels\n",
    "            out_channels (int): Number of output channels\n",
    "        Returns:\n",
    "            torch.nn.Sequential: A sequential block representing the Inception_A Layer\n",
    "        \"\"\"\n",
    "        super(Inception_A, self).__init__()\n",
    "\n",
    "        self.branch1_1 = ConvolutionalLayer.create(in_channels, 32, 1, 1, 0)\n",
    "\n",
    "        self.branch2_1 = ConvolutionalLayer.create(in_channels, 32, 1, 1, 0)\n",
    "        self.branch2_2 = ConvolutionalLayer.create(32, 32, 3, 1, 1)\n",
    "\n",
    "        self.branch3_1 = ConvolutionalLayer.create(in_channels, 32, 1, 1, 0)\n",
    "        self.branch3_2 = ConvolutionalLayer.create(32, 48, 3, 1, 1)\n",
    "        self.branch3_3 = ConvolutionalLayer.create(48, 64, 3, 1, 1)\n",
    "\n",
    "        self.step2 = torch.nn.Conv2d(32 + 32 + 64, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.step3 = CustomReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the Inception-ResNet-V2 layer.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width)\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after passing through all branches of the Inception_A layer\n",
    "        \"\"\"\n",
    "        x_b1 = self.branch1_1(x)\n",
    "\n",
    "        x_b2 = self.branch2_1(x)\n",
    "        x_b2 = self.branch2_2(x_b2)\n",
    "\n",
    "        x_b3 = self.branch3_1(x)\n",
    "        x_b3 = self.branch3_2(x_b3)\n",
    "        x_b3 = self.branch3_3(x_b3)\n",
    "\n",
    "        x_step = torch.cat([x_b1, x_b2, x_b3], dim=1)\n",
    "        x_step = self.step2(x_step)\n",
    "\n",
    "        x += x_step\n",
    "        x = self.step3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f391c851-77a9-4366-91e1-03d99ea0f8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Inception_B(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the Inception_B layer(Base on Figure 17) introduced by Inception-ResNet-V2\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> torch.nn.Sequential:\n",
    "        \"\"\"\n",
    "        Creates a Inception layer with a custom ReLU activation Function.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels\n",
    "            out_channels (int): Number of output channels\n",
    "        Returns:\n",
    "            torch.nn.Sequential: A sequential block representing the Inception_B Layer\n",
    "        \"\"\"\n",
    "        super(Inception_B, self).__init__()\n",
    "\n",
    "        self.branch1_1 = ConvolutionalLayer.create(in_channels, 192, 1, 1, 0)\n",
    "\n",
    "        self.branch2_1 = ConvolutionalLayer.create(in_channels, 128, 1, 1, 0)\n",
    "        self.branch2_2 = ConvolutionalLayer.create(128, 160, (1, 7), 1, (0, 3))\n",
    "        self.branch2_3 = ConvolutionalLayer.create(160, 192, (7, 1), 1, (3, 0))\n",
    "\n",
    "        self.step2 = torch.nn.Conv2d(192 + 192, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.step3 = CustomReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the Inception-ResNet-V2 layer.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width)\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after passing through all branches of the Inception_B layer\n",
    "        \"\"\"\n",
    "        x_b1 = self.branch1_1(x)\n",
    "\n",
    "        x_b2 = self.branch2_1(x)\n",
    "        x_b2 = self.branch2_2(x_b2)\n",
    "        x_b2 = self.branch2_3(x_b2)\n",
    "\n",
    "        x_step = torch.cat([x_b1, x_b2], dim=1)\n",
    "        x_step = self.step2(x_step)\n",
    "\n",
    "        x += x_step\n",
    "        x = self.step3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cba988ff-f4c4-47a6-889e-3fc38889b665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Inception_C(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the Inception_C layer(Base on Figure 19) introduced by Inception-ResNet-V2\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> torch.nn.Sequential:\n",
    "        \"\"\"\n",
    "        Creates a Inception layer with a custom ReLU activation Function.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels\n",
    "            out_channels (int): Number of output channels\n",
    "        Returns:\n",
    "            torch.nn.Sequential: A sequential block representing the Inception_C Layer\n",
    "        \"\"\"\n",
    "        super(Inception_C, self).__init__()\n",
    "\n",
    "        self.branch1_1 = ConvolutionalLayer.create(in_channels, 192, 1, 1, 0)\n",
    "\n",
    "        self.branch2_1 = ConvolutionalLayer.create(in_channels, 192, 1, 1, 0)\n",
    "        self.branch2_2 = ConvolutionalLayer.create(192, 224, (1, 3), 1, (0, 1))\n",
    "        self.branch2_3 = ConvolutionalLayer.create(224, 256, (3, 1), 1, (1, 0))\n",
    "\n",
    "        self.step2 = torch.nn.Conv2d(192 + 256, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.step3 = CustomReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the Inception-ResNet-V1 layer.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width)\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after passing through all branches of the Inception_C layer\n",
    "        \"\"\"\n",
    "        x_b1 = self.branch1_1(x)\n",
    "\n",
    "        x_b2 = self.branch2_1(x)\n",
    "        x_b2 = self.branch2_2(x_b2)\n",
    "        x_b2 = self.branch2_3(x_b2)\n",
    "\n",
    "        x_step = torch.cat([x_b1, x_b2], dim=1)\n",
    "        x_step = self.step2(x_step)\n",
    "\n",
    "        x += x_step\n",
    "        x = self.step3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "615f70b0-5174-4081-9d13-7e5a0ba875e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Reduction_A(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the Reduction_A layer(Base on Figure 7) introduced by Inception V4, Inception-ResNet-V1 and Inception-ResNet-V2\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, n: int, k: int, l: int, m: int) -> torch.nn.Sequential:\n",
    "        \"\"\"\n",
    "        Creates a Inception layer with a custom ReLU activation Function.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels\n",
    "            n (int): The number of filters to output after the 3x3 Convolutions operation in Branch2. (In Paper, 384)\n",
    "            k (int): The number of filters to output after the 1x1 Convolutions operation in Branch3. (In Paper, 256)\n",
    "            l (int): The number of filters to output after the 3x3 Convolutions operation in Branch3. (In Paper, 256)\n",
    "            m (int): The number of filters to output after the 3x3 Convolutions operation(with stride=2) in Branch3. (In Paper, 384)\n",
    "        Returns:\n",
    "            torch.nn.Sequential: A sequential block representing the Reduction_A Layer\n",
    "        \"\"\"\n",
    "        super(Reduction_A, self).__init__()\n",
    "\n",
    "        self.branch1_1 = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "\n",
    "        self.branch2_1 = ConvolutionalLayer.create(in_channels, n, 3, 2, 0)\n",
    "\n",
    "        self.branch3_1 = ConvolutionalLayer.create(in_channels, k, 1, 1, 0)\n",
    "        self.branch3_2 = ConvolutionalLayer.create(k, l, 3, 1, 1)\n",
    "        self.branch3_3 = ConvolutionalLayer.create(l, m, 3, 2, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the Inception V4, Inception-ResNet-V1 and Inception-ResNet-V2 layer.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width)\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after passing through all branches of the Reduction_A layer\n",
    "        \"\"\"\n",
    "        x_b1 = self.branch1_1(x)\n",
    "\n",
    "        x_b2 = self.branch2_1(x)\n",
    "\n",
    "        x_b3 = self.branch3_1(x)\n",
    "        x_b3 = self.branch3_2(x_b3)\n",
    "        x_b3 = self.branch3_3(x_b3)\n",
    "\n",
    "        x = torch.cat([x_b1, x_b2, x_b3], dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28559752-8fd2-43fc-99cf-42deed9b5697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Reduction_B(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the Reduction_B layer(Base on Figure 18) introduced by Inception-ResNet-V2\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> torch.nn.Sequential:\n",
    "        \"\"\"\n",
    "        Creates a Inception layer with a custom ReLU activation Function.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels\n",
    "            out_channels (int): Number of output channels\n",
    "            The value of the out_channels parameter must always be a multiple of 7\n",
    "        Returns:\n",
    "            torch.nn.Sequential: A sequential block representing the Reduction_B Layer\n",
    "        \"\"\"\n",
    "        super(Reduction_B, self).__init__()\n",
    "\n",
    "        self.branch1_1 = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=0)  # 1152\n",
    "\n",
    "        self.branch2_1 = ConvolutionalLayer.create(in_channels, 256, 1, 1, 0)\n",
    "        self.branch2_2 = ConvolutionalLayer.create(256, ((out_channels - in_channels) // 31) * 12, 3, 2, 0)\n",
    "\n",
    "        self.branch3_1 = ConvolutionalLayer.create(in_channels, 256, 1, 1, 0)\n",
    "        self.branch3_2 = ConvolutionalLayer.create(256, ((out_channels - in_channels) // 31) * 9, 3, 2, 0)\n",
    "\n",
    "        self.branch4_1 = ConvolutionalLayer.create(in_channels, 256, 1, 1, 0)\n",
    "        self.branch4_2 = ConvolutionalLayer.create(256, 256, 3, 1, 1)\n",
    "        self.branch4_3 = ConvolutionalLayer.create(256, ((out_channels - in_channels) // 31) * 10, 3, 2, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the Inception-ResNet-V1 layer.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width)\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after passing through all branches of the Reduction_B layer\n",
    "        \"\"\"\n",
    "        x_b1 = self.branch1_1(x)\n",
    "\n",
    "        x_b2 = self.branch2_1(x)\n",
    "        x_b2 = self.branch2_2(x_b2)\n",
    "\n",
    "        x_b3 = self.branch3_1(x)\n",
    "        x_b3 = self.branch3_2(x_b3)\n",
    "\n",
    "        x_b4 = self.branch4_1(x)\n",
    "        x_b4 = self.branch4_2(x_b4)\n",
    "        x_b4 = self.branch4_3(x_b4)\n",
    "\n",
    "        x = torch.cat([x_b1, x_b2, x_b3, x_b4], dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05e28546-0cde-424e-b1b0-ee510af9e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CustomInceptionResNet_V2(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the introduced by Inception-ResNet-V2 model.\n",
    "    Input: Image tensor (batch_size, 3, 299, 299)\n",
    "    Output: Class scores (batch_size, 1000)\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_rate=0.8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dropout_rate (float): Dropout Rate = 0.8(Base on Paper)\n",
    "        \"\"\"\n",
    "        super(CustomInceptionResNet_V2, self).__init__()\n",
    "\n",
    "        # Convolutional and pooling layers\n",
    "        self.layer1 = Stem(3, 384)\n",
    "\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            *[Inception_A(384, 384) for _ in range(5)]\n",
    "        )\n",
    "\n",
    "        self.layer3 = Reduction_A(384, 384, 256, 256, 384)\n",
    "\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            *[Inception_B(1152, 1152) for _ in range(10)]\n",
    "        )\n",
    "\n",
    "        self.layer5 = Reduction_B(1152, 2144)\n",
    "\n",
    "        self.layer6 = torch.nn.Sequential(\n",
    "            *[Inception_C(2144, 2144) for _ in range(5)]\n",
    "        )\n",
    "\n",
    "        self.layer7 = torch.nn.AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
    "        \n",
    "\n",
    "        # Fully Connected layers and dropout\n",
    "        self.layer_drop = torch.nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        self.layer8 = FullyConnectedLayer.Dense(2144, 1000)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input image tensor (batch_size, 3, 299, 299)\n",
    "        Returns:\n",
    "            torch.Tensor: Class scores (batch_size, 1000)\n",
    "        \"\"\"\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "        x = self.layer7(x)\n",
    "        x = self.layer_drop(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.layer8(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dde87c1a-4705-4945-ba54-bb40126f0140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomInceptionResNet_V2\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 149, 149]             896\n",
      "       BatchNorm2d-2         [-1, 32, 149, 149]              64\n",
      "        CustomReLU-3         [-1, 32, 149, 149]               0\n",
      "            Conv2d-4         [-1, 32, 147, 147]           9,248\n",
      "       BatchNorm2d-5         [-1, 32, 147, 147]              64\n",
      "        CustomReLU-6         [-1, 32, 147, 147]               0\n",
      "            Conv2d-7         [-1, 64, 147, 147]          18,496\n",
      "       BatchNorm2d-8         [-1, 64, 147, 147]             128\n",
      "        CustomReLU-9         [-1, 64, 147, 147]               0\n",
      "        MaxPool2d-10           [-1, 64, 73, 73]               0\n",
      "           Conv2d-11           [-1, 96, 73, 73]          55,392\n",
      "      BatchNorm2d-12           [-1, 96, 73, 73]             192\n",
      "       CustomReLU-13           [-1, 96, 73, 73]               0\n",
      "           Conv2d-14           [-1, 64, 73, 73]          10,304\n",
      "      BatchNorm2d-15           [-1, 64, 73, 73]             128\n",
      "       CustomReLU-16           [-1, 64, 73, 73]               0\n",
      "           Conv2d-17           [-1, 96, 71, 71]          55,392\n",
      "      BatchNorm2d-18           [-1, 96, 71, 71]             192\n",
      "       CustomReLU-19           [-1, 96, 71, 71]               0\n",
      "           Conv2d-20           [-1, 64, 73, 73]          10,304\n",
      "      BatchNorm2d-21           [-1, 64, 73, 73]             128\n",
      "       CustomReLU-22           [-1, 64, 73, 73]               0\n",
      "           Conv2d-23           [-1, 64, 73, 73]          28,736\n",
      "      BatchNorm2d-24           [-1, 64, 73, 73]             128\n",
      "       CustomReLU-25           [-1, 64, 73, 73]               0\n",
      "           Conv2d-26           [-1, 64, 73, 73]          28,736\n",
      "      BatchNorm2d-27           [-1, 64, 73, 73]             128\n",
      "       CustomReLU-28           [-1, 64, 73, 73]               0\n",
      "           Conv2d-29           [-1, 96, 71, 71]          55,392\n",
      "      BatchNorm2d-30           [-1, 96, 71, 71]             192\n",
      "       CustomReLU-31           [-1, 96, 71, 71]               0\n",
      "           Conv2d-32          [-1, 192, 35, 35]         331,968\n",
      "      BatchNorm2d-33          [-1, 192, 35, 35]             384\n",
      "       CustomReLU-34          [-1, 192, 35, 35]               0\n",
      "        MaxPool2d-35          [-1, 192, 35, 35]               0\n",
      "             Stem-36          [-1, 384, 35, 35]               0\n",
      "           Conv2d-37           [-1, 32, 35, 35]          12,320\n",
      "      BatchNorm2d-38           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-39           [-1, 32, 35, 35]               0\n",
      "           Conv2d-40           [-1, 32, 35, 35]          12,320\n",
      "      BatchNorm2d-41           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-42           [-1, 32, 35, 35]               0\n",
      "           Conv2d-43           [-1, 32, 35, 35]           9,248\n",
      "      BatchNorm2d-44           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-45           [-1, 32, 35, 35]               0\n",
      "           Conv2d-46           [-1, 32, 35, 35]          12,320\n",
      "      BatchNorm2d-47           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-48           [-1, 32, 35, 35]               0\n",
      "           Conv2d-49           [-1, 48, 35, 35]          13,872\n",
      "      BatchNorm2d-50           [-1, 48, 35, 35]              96\n",
      "       CustomReLU-51           [-1, 48, 35, 35]               0\n",
      "           Conv2d-52           [-1, 64, 35, 35]          27,712\n",
      "      BatchNorm2d-53           [-1, 64, 35, 35]             128\n",
      "       CustomReLU-54           [-1, 64, 35, 35]               0\n",
      "           Conv2d-55          [-1, 384, 35, 35]          49,536\n",
      "       CustomReLU-56          [-1, 384, 35, 35]               0\n",
      "      Inception_A-57          [-1, 384, 35, 35]               0\n",
      "           Conv2d-58           [-1, 32, 35, 35]          12,320\n",
      "      BatchNorm2d-59           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-60           [-1, 32, 35, 35]               0\n",
      "           Conv2d-61           [-1, 32, 35, 35]          12,320\n",
      "      BatchNorm2d-62           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-63           [-1, 32, 35, 35]               0\n",
      "           Conv2d-64           [-1, 32, 35, 35]           9,248\n",
      "      BatchNorm2d-65           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-66           [-1, 32, 35, 35]               0\n",
      "           Conv2d-67           [-1, 32, 35, 35]          12,320\n",
      "      BatchNorm2d-68           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-69           [-1, 32, 35, 35]               0\n",
      "           Conv2d-70           [-1, 48, 35, 35]          13,872\n",
      "      BatchNorm2d-71           [-1, 48, 35, 35]              96\n",
      "       CustomReLU-72           [-1, 48, 35, 35]               0\n",
      "           Conv2d-73           [-1, 64, 35, 35]          27,712\n",
      "      BatchNorm2d-74           [-1, 64, 35, 35]             128\n",
      "       CustomReLU-75           [-1, 64, 35, 35]               0\n",
      "           Conv2d-76          [-1, 384, 35, 35]          49,536\n",
      "       CustomReLU-77          [-1, 384, 35, 35]               0\n",
      "      Inception_A-78          [-1, 384, 35, 35]               0\n",
      "           Conv2d-79           [-1, 32, 35, 35]          12,320\n",
      "      BatchNorm2d-80           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-81           [-1, 32, 35, 35]               0\n",
      "           Conv2d-82           [-1, 32, 35, 35]          12,320\n",
      "      BatchNorm2d-83           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-84           [-1, 32, 35, 35]               0\n",
      "           Conv2d-85           [-1, 32, 35, 35]           9,248\n",
      "      BatchNorm2d-86           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-87           [-1, 32, 35, 35]               0\n",
      "           Conv2d-88           [-1, 32, 35, 35]          12,320\n",
      "      BatchNorm2d-89           [-1, 32, 35, 35]              64\n",
      "       CustomReLU-90           [-1, 32, 35, 35]               0\n",
      "           Conv2d-91           [-1, 48, 35, 35]          13,872\n",
      "      BatchNorm2d-92           [-1, 48, 35, 35]              96\n",
      "       CustomReLU-93           [-1, 48, 35, 35]               0\n",
      "           Conv2d-94           [-1, 64, 35, 35]          27,712\n",
      "      BatchNorm2d-95           [-1, 64, 35, 35]             128\n",
      "       CustomReLU-96           [-1, 64, 35, 35]               0\n",
      "           Conv2d-97          [-1, 384, 35, 35]          49,536\n",
      "       CustomReLU-98          [-1, 384, 35, 35]               0\n",
      "      Inception_A-99          [-1, 384, 35, 35]               0\n",
      "          Conv2d-100           [-1, 32, 35, 35]          12,320\n",
      "     BatchNorm2d-101           [-1, 32, 35, 35]              64\n",
      "      CustomReLU-102           [-1, 32, 35, 35]               0\n",
      "          Conv2d-103           [-1, 32, 35, 35]          12,320\n",
      "     BatchNorm2d-104           [-1, 32, 35, 35]              64\n",
      "      CustomReLU-105           [-1, 32, 35, 35]               0\n",
      "          Conv2d-106           [-1, 32, 35, 35]           9,248\n",
      "     BatchNorm2d-107           [-1, 32, 35, 35]              64\n",
      "      CustomReLU-108           [-1, 32, 35, 35]               0\n",
      "          Conv2d-109           [-1, 32, 35, 35]          12,320\n",
      "     BatchNorm2d-110           [-1, 32, 35, 35]              64\n",
      "      CustomReLU-111           [-1, 32, 35, 35]               0\n",
      "          Conv2d-112           [-1, 48, 35, 35]          13,872\n",
      "     BatchNorm2d-113           [-1, 48, 35, 35]              96\n",
      "      CustomReLU-114           [-1, 48, 35, 35]               0\n",
      "          Conv2d-115           [-1, 64, 35, 35]          27,712\n",
      "     BatchNorm2d-116           [-1, 64, 35, 35]             128\n",
      "      CustomReLU-117           [-1, 64, 35, 35]               0\n",
      "          Conv2d-118          [-1, 384, 35, 35]          49,536\n",
      "      CustomReLU-119          [-1, 384, 35, 35]               0\n",
      "     Inception_A-120          [-1, 384, 35, 35]               0\n",
      "          Conv2d-121           [-1, 32, 35, 35]          12,320\n",
      "     BatchNorm2d-122           [-1, 32, 35, 35]              64\n",
      "      CustomReLU-123           [-1, 32, 35, 35]               0\n",
      "          Conv2d-124           [-1, 32, 35, 35]          12,320\n",
      "     BatchNorm2d-125           [-1, 32, 35, 35]              64\n",
      "      CustomReLU-126           [-1, 32, 35, 35]               0\n",
      "          Conv2d-127           [-1, 32, 35, 35]           9,248\n",
      "     BatchNorm2d-128           [-1, 32, 35, 35]              64\n",
      "      CustomReLU-129           [-1, 32, 35, 35]               0\n",
      "          Conv2d-130           [-1, 32, 35, 35]          12,320\n",
      "     BatchNorm2d-131           [-1, 32, 35, 35]              64\n",
      "      CustomReLU-132           [-1, 32, 35, 35]               0\n",
      "          Conv2d-133           [-1, 48, 35, 35]          13,872\n",
      "     BatchNorm2d-134           [-1, 48, 35, 35]              96\n",
      "      CustomReLU-135           [-1, 48, 35, 35]               0\n",
      "          Conv2d-136           [-1, 64, 35, 35]          27,712\n",
      "     BatchNorm2d-137           [-1, 64, 35, 35]             128\n",
      "      CustomReLU-138           [-1, 64, 35, 35]               0\n",
      "          Conv2d-139          [-1, 384, 35, 35]          49,536\n",
      "      CustomReLU-140          [-1, 384, 35, 35]               0\n",
      "     Inception_A-141          [-1, 384, 35, 35]               0\n",
      "       MaxPool2d-142          [-1, 384, 17, 17]               0\n",
      "          Conv2d-143          [-1, 384, 17, 17]       1,327,488\n",
      "     BatchNorm2d-144          [-1, 384, 17, 17]             768\n",
      "      CustomReLU-145          [-1, 384, 17, 17]               0\n",
      "          Conv2d-146          [-1, 256, 35, 35]          98,560\n",
      "     BatchNorm2d-147          [-1, 256, 35, 35]             512\n",
      "      CustomReLU-148          [-1, 256, 35, 35]               0\n",
      "          Conv2d-149          [-1, 256, 35, 35]         590,080\n",
      "     BatchNorm2d-150          [-1, 256, 35, 35]             512\n",
      "      CustomReLU-151          [-1, 256, 35, 35]               0\n",
      "          Conv2d-152          [-1, 384, 17, 17]         885,120\n",
      "     BatchNorm2d-153          [-1, 384, 17, 17]             768\n",
      "      CustomReLU-154          [-1, 384, 17, 17]               0\n",
      "     Reduction_A-155         [-1, 1152, 17, 17]               0\n",
      "          Conv2d-156          [-1, 192, 17, 17]         221,376\n",
      "     BatchNorm2d-157          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-158          [-1, 192, 17, 17]               0\n",
      "          Conv2d-159          [-1, 128, 17, 17]         147,584\n",
      "     BatchNorm2d-160          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-161          [-1, 128, 17, 17]               0\n",
      "          Conv2d-162          [-1, 160, 17, 17]         143,520\n",
      "     BatchNorm2d-163          [-1, 160, 17, 17]             320\n",
      "      CustomReLU-164          [-1, 160, 17, 17]               0\n",
      "          Conv2d-165          [-1, 192, 17, 17]         215,232\n",
      "     BatchNorm2d-166          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-167          [-1, 192, 17, 17]               0\n",
      "          Conv2d-168         [-1, 1152, 17, 17]         443,520\n",
      "      CustomReLU-169         [-1, 1152, 17, 17]               0\n",
      "     Inception_B-170         [-1, 1152, 17, 17]               0\n",
      "          Conv2d-171          [-1, 192, 17, 17]         221,376\n",
      "     BatchNorm2d-172          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-173          [-1, 192, 17, 17]               0\n",
      "          Conv2d-174          [-1, 128, 17, 17]         147,584\n",
      "     BatchNorm2d-175          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-176          [-1, 128, 17, 17]               0\n",
      "          Conv2d-177          [-1, 160, 17, 17]         143,520\n",
      "     BatchNorm2d-178          [-1, 160, 17, 17]             320\n",
      "      CustomReLU-179          [-1, 160, 17, 17]               0\n",
      "          Conv2d-180          [-1, 192, 17, 17]         215,232\n",
      "     BatchNorm2d-181          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-182          [-1, 192, 17, 17]               0\n",
      "          Conv2d-183         [-1, 1152, 17, 17]         443,520\n",
      "      CustomReLU-184         [-1, 1152, 17, 17]               0\n",
      "     Inception_B-185         [-1, 1152, 17, 17]               0\n",
      "          Conv2d-186          [-1, 192, 17, 17]         221,376\n",
      "     BatchNorm2d-187          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-188          [-1, 192, 17, 17]               0\n",
      "          Conv2d-189          [-1, 128, 17, 17]         147,584\n",
      "     BatchNorm2d-190          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-191          [-1, 128, 17, 17]               0\n",
      "          Conv2d-192          [-1, 160, 17, 17]         143,520\n",
      "     BatchNorm2d-193          [-1, 160, 17, 17]             320\n",
      "      CustomReLU-194          [-1, 160, 17, 17]               0\n",
      "          Conv2d-195          [-1, 192, 17, 17]         215,232\n",
      "     BatchNorm2d-196          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-197          [-1, 192, 17, 17]               0\n",
      "          Conv2d-198         [-1, 1152, 17, 17]         443,520\n",
      "      CustomReLU-199         [-1, 1152, 17, 17]               0\n",
      "     Inception_B-200         [-1, 1152, 17, 17]               0\n",
      "          Conv2d-201          [-1, 192, 17, 17]         221,376\n",
      "     BatchNorm2d-202          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-203          [-1, 192, 17, 17]               0\n",
      "          Conv2d-204          [-1, 128, 17, 17]         147,584\n",
      "     BatchNorm2d-205          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-206          [-1, 128, 17, 17]               0\n",
      "          Conv2d-207          [-1, 160, 17, 17]         143,520\n",
      "     BatchNorm2d-208          [-1, 160, 17, 17]             320\n",
      "      CustomReLU-209          [-1, 160, 17, 17]               0\n",
      "          Conv2d-210          [-1, 192, 17, 17]         215,232\n",
      "     BatchNorm2d-211          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-212          [-1, 192, 17, 17]               0\n",
      "          Conv2d-213         [-1, 1152, 17, 17]         443,520\n",
      "      CustomReLU-214         [-1, 1152, 17, 17]               0\n",
      "     Inception_B-215         [-1, 1152, 17, 17]               0\n",
      "          Conv2d-216          [-1, 192, 17, 17]         221,376\n",
      "     BatchNorm2d-217          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-218          [-1, 192, 17, 17]               0\n",
      "          Conv2d-219          [-1, 128, 17, 17]         147,584\n",
      "     BatchNorm2d-220          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-221          [-1, 128, 17, 17]               0\n",
      "          Conv2d-222          [-1, 160, 17, 17]         143,520\n",
      "     BatchNorm2d-223          [-1, 160, 17, 17]             320\n",
      "      CustomReLU-224          [-1, 160, 17, 17]               0\n",
      "          Conv2d-225          [-1, 192, 17, 17]         215,232\n",
      "     BatchNorm2d-226          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-227          [-1, 192, 17, 17]               0\n",
      "          Conv2d-228         [-1, 1152, 17, 17]         443,520\n",
      "      CustomReLU-229         [-1, 1152, 17, 17]               0\n",
      "     Inception_B-230         [-1, 1152, 17, 17]               0\n",
      "          Conv2d-231          [-1, 192, 17, 17]         221,376\n",
      "     BatchNorm2d-232          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-233          [-1, 192, 17, 17]               0\n",
      "          Conv2d-234          [-1, 128, 17, 17]         147,584\n",
      "     BatchNorm2d-235          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-236          [-1, 128, 17, 17]               0\n",
      "          Conv2d-237          [-1, 160, 17, 17]         143,520\n",
      "     BatchNorm2d-238          [-1, 160, 17, 17]             320\n",
      "      CustomReLU-239          [-1, 160, 17, 17]               0\n",
      "          Conv2d-240          [-1, 192, 17, 17]         215,232\n",
      "     BatchNorm2d-241          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-242          [-1, 192, 17, 17]               0\n",
      "          Conv2d-243         [-1, 1152, 17, 17]         443,520\n",
      "      CustomReLU-244         [-1, 1152, 17, 17]               0\n",
      "     Inception_B-245         [-1, 1152, 17, 17]               0\n",
      "          Conv2d-246          [-1, 192, 17, 17]         221,376\n",
      "     BatchNorm2d-247          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-248          [-1, 192, 17, 17]               0\n",
      "          Conv2d-249          [-1, 128, 17, 17]         147,584\n",
      "     BatchNorm2d-250          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-251          [-1, 128, 17, 17]               0\n",
      "          Conv2d-252          [-1, 160, 17, 17]         143,520\n",
      "     BatchNorm2d-253          [-1, 160, 17, 17]             320\n",
      "      CustomReLU-254          [-1, 160, 17, 17]               0\n",
      "          Conv2d-255          [-1, 192, 17, 17]         215,232\n",
      "     BatchNorm2d-256          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-257          [-1, 192, 17, 17]               0\n",
      "          Conv2d-258         [-1, 1152, 17, 17]         443,520\n",
      "      CustomReLU-259         [-1, 1152, 17, 17]               0\n",
      "     Inception_B-260         [-1, 1152, 17, 17]               0\n",
      "          Conv2d-261          [-1, 192, 17, 17]         221,376\n",
      "     BatchNorm2d-262          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-263          [-1, 192, 17, 17]               0\n",
      "          Conv2d-264          [-1, 128, 17, 17]         147,584\n",
      "     BatchNorm2d-265          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-266          [-1, 128, 17, 17]               0\n",
      "          Conv2d-267          [-1, 160, 17, 17]         143,520\n",
      "     BatchNorm2d-268          [-1, 160, 17, 17]             320\n",
      "      CustomReLU-269          [-1, 160, 17, 17]               0\n",
      "          Conv2d-270          [-1, 192, 17, 17]         215,232\n",
      "     BatchNorm2d-271          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-272          [-1, 192, 17, 17]               0\n",
      "          Conv2d-273         [-1, 1152, 17, 17]         443,520\n",
      "      CustomReLU-274         [-1, 1152, 17, 17]               0\n",
      "     Inception_B-275         [-1, 1152, 17, 17]               0\n",
      "          Conv2d-276          [-1, 192, 17, 17]         221,376\n",
      "     BatchNorm2d-277          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-278          [-1, 192, 17, 17]               0\n",
      "          Conv2d-279          [-1, 128, 17, 17]         147,584\n",
      "     BatchNorm2d-280          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-281          [-1, 128, 17, 17]               0\n",
      "          Conv2d-282          [-1, 160, 17, 17]         143,520\n",
      "     BatchNorm2d-283          [-1, 160, 17, 17]             320\n",
      "      CustomReLU-284          [-1, 160, 17, 17]               0\n",
      "          Conv2d-285          [-1, 192, 17, 17]         215,232\n",
      "     BatchNorm2d-286          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-287          [-1, 192, 17, 17]               0\n",
      "          Conv2d-288         [-1, 1152, 17, 17]         443,520\n",
      "      CustomReLU-289         [-1, 1152, 17, 17]               0\n",
      "     Inception_B-290         [-1, 1152, 17, 17]               0\n",
      "          Conv2d-291          [-1, 192, 17, 17]         221,376\n",
      "     BatchNorm2d-292          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-293          [-1, 192, 17, 17]               0\n",
      "          Conv2d-294          [-1, 128, 17, 17]         147,584\n",
      "     BatchNorm2d-295          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-296          [-1, 128, 17, 17]               0\n",
      "          Conv2d-297          [-1, 160, 17, 17]         143,520\n",
      "     BatchNorm2d-298          [-1, 160, 17, 17]             320\n",
      "      CustomReLU-299          [-1, 160, 17, 17]               0\n",
      "          Conv2d-300          [-1, 192, 17, 17]         215,232\n",
      "     BatchNorm2d-301          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-302          [-1, 192, 17, 17]               0\n",
      "          Conv2d-303         [-1, 1152, 17, 17]         443,520\n",
      "      CustomReLU-304         [-1, 1152, 17, 17]               0\n",
      "     Inception_B-305         [-1, 1152, 17, 17]               0\n",
      "       MaxPool2d-306           [-1, 1152, 8, 8]               0\n",
      "          Conv2d-307          [-1, 256, 17, 17]         295,168\n",
      "     BatchNorm2d-308          [-1, 256, 17, 17]             512\n",
      "      CustomReLU-309          [-1, 256, 17, 17]               0\n",
      "          Conv2d-310            [-1, 384, 8, 8]         885,120\n",
      "     BatchNorm2d-311            [-1, 384, 8, 8]             768\n",
      "      CustomReLU-312            [-1, 384, 8, 8]               0\n",
      "          Conv2d-313          [-1, 256, 17, 17]         295,168\n",
      "     BatchNorm2d-314          [-1, 256, 17, 17]             512\n",
      "      CustomReLU-315          [-1, 256, 17, 17]               0\n",
      "          Conv2d-316            [-1, 288, 8, 8]         663,840\n",
      "     BatchNorm2d-317            [-1, 288, 8, 8]             576\n",
      "      CustomReLU-318            [-1, 288, 8, 8]               0\n",
      "          Conv2d-319          [-1, 256, 17, 17]         295,168\n",
      "     BatchNorm2d-320          [-1, 256, 17, 17]             512\n",
      "      CustomReLU-321          [-1, 256, 17, 17]               0\n",
      "          Conv2d-322          [-1, 256, 17, 17]         590,080\n",
      "     BatchNorm2d-323          [-1, 256, 17, 17]             512\n",
      "      CustomReLU-324          [-1, 256, 17, 17]               0\n",
      "          Conv2d-325            [-1, 320, 8, 8]         737,600\n",
      "     BatchNorm2d-326            [-1, 320, 8, 8]             640\n",
      "      CustomReLU-327            [-1, 320, 8, 8]               0\n",
      "     Reduction_B-328           [-1, 2144, 8, 8]               0\n",
      "          Conv2d-329            [-1, 192, 8, 8]         411,840\n",
      "     BatchNorm2d-330            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-331            [-1, 192, 8, 8]               0\n",
      "          Conv2d-332            [-1, 192, 8, 8]         411,840\n",
      "     BatchNorm2d-333            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-334            [-1, 192, 8, 8]               0\n",
      "          Conv2d-335            [-1, 224, 8, 8]         129,248\n",
      "     BatchNorm2d-336            [-1, 224, 8, 8]             448\n",
      "      CustomReLU-337            [-1, 224, 8, 8]               0\n",
      "          Conv2d-338            [-1, 256, 8, 8]         172,288\n",
      "     BatchNorm2d-339            [-1, 256, 8, 8]             512\n",
      "      CustomReLU-340            [-1, 256, 8, 8]               0\n",
      "          Conv2d-341           [-1, 2144, 8, 8]         962,656\n",
      "      CustomReLU-342           [-1, 2144, 8, 8]               0\n",
      "     Inception_C-343           [-1, 2144, 8, 8]               0\n",
      "          Conv2d-344            [-1, 192, 8, 8]         411,840\n",
      "     BatchNorm2d-345            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-346            [-1, 192, 8, 8]               0\n",
      "          Conv2d-347            [-1, 192, 8, 8]         411,840\n",
      "     BatchNorm2d-348            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-349            [-1, 192, 8, 8]               0\n",
      "          Conv2d-350            [-1, 224, 8, 8]         129,248\n",
      "     BatchNorm2d-351            [-1, 224, 8, 8]             448\n",
      "      CustomReLU-352            [-1, 224, 8, 8]               0\n",
      "          Conv2d-353            [-1, 256, 8, 8]         172,288\n",
      "     BatchNorm2d-354            [-1, 256, 8, 8]             512\n",
      "      CustomReLU-355            [-1, 256, 8, 8]               0\n",
      "          Conv2d-356           [-1, 2144, 8, 8]         962,656\n",
      "      CustomReLU-357           [-1, 2144, 8, 8]               0\n",
      "     Inception_C-358           [-1, 2144, 8, 8]               0\n",
      "          Conv2d-359            [-1, 192, 8, 8]         411,840\n",
      "     BatchNorm2d-360            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-361            [-1, 192, 8, 8]               0\n",
      "          Conv2d-362            [-1, 192, 8, 8]         411,840\n",
      "     BatchNorm2d-363            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-364            [-1, 192, 8, 8]               0\n",
      "          Conv2d-365            [-1, 224, 8, 8]         129,248\n",
      "     BatchNorm2d-366            [-1, 224, 8, 8]             448\n",
      "      CustomReLU-367            [-1, 224, 8, 8]               0\n",
      "          Conv2d-368            [-1, 256, 8, 8]         172,288\n",
      "     BatchNorm2d-369            [-1, 256, 8, 8]             512\n",
      "      CustomReLU-370            [-1, 256, 8, 8]               0\n",
      "          Conv2d-371           [-1, 2144, 8, 8]         962,656\n",
      "      CustomReLU-372           [-1, 2144, 8, 8]               0\n",
      "     Inception_C-373           [-1, 2144, 8, 8]               0\n",
      "          Conv2d-374            [-1, 192, 8, 8]         411,840\n",
      "     BatchNorm2d-375            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-376            [-1, 192, 8, 8]               0\n",
      "          Conv2d-377            [-1, 192, 8, 8]         411,840\n",
      "     BatchNorm2d-378            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-379            [-1, 192, 8, 8]               0\n",
      "          Conv2d-380            [-1, 224, 8, 8]         129,248\n",
      "     BatchNorm2d-381            [-1, 224, 8, 8]             448\n",
      "      CustomReLU-382            [-1, 224, 8, 8]               0\n",
      "          Conv2d-383            [-1, 256, 8, 8]         172,288\n",
      "     BatchNorm2d-384            [-1, 256, 8, 8]             512\n",
      "      CustomReLU-385            [-1, 256, 8, 8]               0\n",
      "          Conv2d-386           [-1, 2144, 8, 8]         962,656\n",
      "      CustomReLU-387           [-1, 2144, 8, 8]               0\n",
      "     Inception_C-388           [-1, 2144, 8, 8]               0\n",
      "          Conv2d-389            [-1, 192, 8, 8]         411,840\n",
      "     BatchNorm2d-390            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-391            [-1, 192, 8, 8]               0\n",
      "          Conv2d-392            [-1, 192, 8, 8]         411,840\n",
      "     BatchNorm2d-393            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-394            [-1, 192, 8, 8]               0\n",
      "          Conv2d-395            [-1, 224, 8, 8]         129,248\n",
      "     BatchNorm2d-396            [-1, 224, 8, 8]             448\n",
      "      CustomReLU-397            [-1, 224, 8, 8]               0\n",
      "          Conv2d-398            [-1, 256, 8, 8]         172,288\n",
      "     BatchNorm2d-399            [-1, 256, 8, 8]             512\n",
      "      CustomReLU-400            [-1, 256, 8, 8]               0\n",
      "          Conv2d-401           [-1, 2144, 8, 8]         962,656\n",
      "      CustomReLU-402           [-1, 2144, 8, 8]               0\n",
      "     Inception_C-403           [-1, 2144, 8, 8]               0\n",
      "       AvgPool2d-404           [-1, 2144, 1, 1]               0\n",
      "         Dropout-405           [-1, 2144, 1, 1]               0\n",
      "          Linear-406                 [-1, 1000]       2,145,000\n",
      "================================================================\n",
      "Total params: 32,284,376\n",
      "Trainable params: 32,284,376\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.02\n",
      "Forward/backward pass size (MB): 405.06\n",
      "Params size (MB): 123.16\n",
      "Estimated Total Size (MB): 529.24\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "model = CustomInceptionResNet_V2()\n",
    "\n",
    "print(model.__class__.__name__)\n",
    "summary(model, input_size=(3, 299, 299))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "afa7d470-8b23-408b-91f6-b1bc4bfd0afc",
   "metadata": {},
   "source": [
    "# Create a dummy input tensor (adjust its shape to match model)\n",
    "dummy_input = torch.randn(1, 3, 299, 299)  # Shape: (batch_size, channels, height, width)\n",
    "    \n",
    "# Initialize the exporter with the model name and Export the model\n",
    "exporter = ModelExporter(model.__class__.__name__)\n",
    "exporter.export_to_onnx(model, dummy_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
