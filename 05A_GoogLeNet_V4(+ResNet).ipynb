{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a141462-e8bc-4727-8a7f-2df88e1dba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CustomReLU(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Custom implementation of the ReLU activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CustomReLU, self).__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Applies the ReLU function element-wise: max(0, x).\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor with ReALU applied\n",
    "        \"\"\"\n",
    "        return torch.maximum(x, torch.zeros_like(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73ef2b6b-0f07-4601-a602-44174a8eb473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ConvolutionalLayer:\n",
    "    \"\"\"\n",
    "    A utility class to create a convolutional layer with a custom ReLU activation.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def create(in_channels: int, out_channels: int, kernel_size: int, stride: int, padding: int) -> torch.nn.Sequential:\n",
    "        \"\"\"\n",
    "        Creates a convolutional layer with a custom ReLU activation.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels\n",
    "            out_channels (int): Number of output channels\n",
    "            kernel_size (int): Size of the convolution kernel\n",
    "            stride (int): Stride of the convolution\n",
    "            padding (int): Padding added to the input\n",
    "        Returns:\n",
    "            torch.nn.Sequential: A sequential layer containing Conv2d and CustomReLU(+Batch Normalization)\n",
    "        \"\"\"\n",
    "        return torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            CustomReLU()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2efcacde-ae2d-4e7d-bc5c-f35a806c0843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class FullyConnectedLayer:\n",
    "    \"\"\"\n",
    "    A utility class to create Fully Connected (FC) layers with Xavier initialization.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def Dense(input_dim: int, output_dim: int) -> torch.nn.Linear:\n",
    "        \"\"\"\n",
    "        Creates an FC layer and applies Xavier initialization.\n",
    "        Args:\n",
    "            input_dim (int): Number of input features\n",
    "            output_dim (int): Number of output features\n",
    "        Returns:\n",
    "            torch.nn.Linear: Initialized Fully-Connected layer\n",
    "        \"\"\"\n",
    "        layer = torch.nn.Linear(input_dim, output_dim, bias=True)\n",
    "        torch.nn.init.xavier_uniform_(layer.weight)\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "165f11a9-0b07-410b-92d9-38795e8d27f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Stem(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the Stem layer(Base on Figure 3) introduced by GoogleNet V4\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> torch.nn.Sequential:\n",
    "        \"\"\"\n",
    "        Creates a Inception layer with a custom ReLU activation Function.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels\n",
    "            out_channels (int): Number of output channels\n",
    "        Returns:\n",
    "            torch.nn.Sequential: A sequential block representing the Stem Layer\n",
    "        \"\"\"\n",
    "        super(Stem, self).__init__()\n",
    "\n",
    "        self.step1 = ConvolutionalLayer.create(in_channels, 32, 3, 2, 0)\n",
    "\n",
    "        self.step2 = ConvolutionalLayer.create(32, 32, 3, 1, 0)\n",
    "\n",
    "        self.step3 = ConvolutionalLayer.create(32, 64, 3, 1, 1)\n",
    "\n",
    "        self.step4_branch_a1 = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "        self.step4_branch_b1 = ConvolutionalLayer.create(64, 96, 3, 2, 0)\n",
    "\n",
    "        self.step5_branch_a1 = ConvolutionalLayer.create(160, 64, 1, 1, 0)\n",
    "        self.step5_branch_a2 = ConvolutionalLayer.create(64, 96, 3, 1, 0)\n",
    "        self.step5_branch_b1 = ConvolutionalLayer.create(160, 64, 1, 1, 0)\n",
    "        self.step5_branch_b2 = ConvolutionalLayer.create(64, 64, (1, 7), 1, (0, 3))\n",
    "        self.step5_branch_b3 = ConvolutionalLayer.create(64, 64, (7, 1), 1, (3, 0))\n",
    "        self.step5_branch_b4 = ConvolutionalLayer.create(64, 96, 3, 1, 0)\n",
    "\n",
    "        self.step6_branch_a1 = ConvolutionalLayer.create(192, out_channels - 192, 3, 2, 0)  # In Paper, (192,192,3,2,0)\n",
    "        self.step6_branch_b1 = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the InceptionV1 layer.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width)\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after passing through all branches of the Stem layer\n",
    "        \"\"\"\n",
    "        x = self.step1(x)\n",
    "        x = self.step2(x)\n",
    "        x = self.step3(x)\n",
    "\n",
    "        x_b1 = self.step4_branch_a1(x)\n",
    "        x_b2 = self.step4_branch_b1(x)\n",
    "        x = torch.cat([x_b1, x_b2], dim=1)\n",
    "\n",
    "        x_b1 = self.step5_branch_a1(x)\n",
    "        x_b1 = self.step5_branch_a2(x_b1)\n",
    "        x_b2 = self.step5_branch_b1(x)\n",
    "        x_b2 = self.step5_branch_b2(x_b2)\n",
    "        x_b2 = self.step5_branch_b3(x_b2)\n",
    "        x_b2 = self.step5_branch_b4(x_b2)\n",
    "        x = torch.cat([x_b1, x_b2], dim=1)\n",
    "\n",
    "        x_b1 = self.step6_branch_a1(x)\n",
    "        x_b2 = self.step6_branch_b1(x)\n",
    "        x = torch.cat([x_b1, x_b2], dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebe6c5db-b839-46a5-8caa-a081eeb90972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Inception_A(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the Inception_A layer(Base on Figure 4) introduced by GoogleNet V4\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> torch.nn.Sequential:\n",
    "        \"\"\"\n",
    "        Creates a Inception layer with a custom ReLU activation Function.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels\n",
    "            out_channels (int): Number of output channels\n",
    "            The value of the out_channels parameter must always be a multiple of 4.\n",
    "        Returns:\n",
    "            torch.nn.Sequential: A sequential block representing the Inception_A Layer\n",
    "        \"\"\"\n",
    "        super(Inception_A, self).__init__()\n",
    "\n",
    "        self.branch1_1 = torch.nn.AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
    "        self.branch1_2 = ConvolutionalLayer.create(in_channels, out_channels // 4, 1, 1, 0)\n",
    "\n",
    "        self.branch2_1 = ConvolutionalLayer.create(in_channels, out_channels // 4, 1, 1, 0)\n",
    "\n",
    "        self.branch3_1 = ConvolutionalLayer.create(in_channels, 64, 1, 1, 0)\n",
    "        self.branch3_2 = ConvolutionalLayer.create(64, out_channels // 4, 3, 1, 1)\n",
    "\n",
    "        self.branch4_1 = ConvolutionalLayer.create(in_channels, 64, 1, 1, 0)\n",
    "        self.branch4_2 = ConvolutionalLayer.create(64, 96, 3, 1, 1)\n",
    "        self.branch4_3 = ConvolutionalLayer.create(96, out_channels // 4, 3, 1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the InceptionV1 layer.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width)\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after passing through all branches of the Inception_A layer\n",
    "        \"\"\"\n",
    "        x_b1 = self.branch1_1(x)\n",
    "        x_b1 = self.branch1_2(x_b1)\n",
    "\n",
    "        x_b2 = self.branch2_1(x)\n",
    "\n",
    "        x_b3 = self.branch3_1(x)\n",
    "        x_b3 = self.branch3_2(x_b3)\n",
    "\n",
    "        x_b4 = self.branch4_1(x)\n",
    "        x_b4 = self.branch4_2(x_b4)\n",
    "        x_b4 = self.branch4_3(x_b4)\n",
    "\n",
    "        x = torch.cat([x_b1, x_b2, x_b3, x_b4], dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f391c851-77a9-4366-91e1-03d99ea0f8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Inception_B(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the Inception_B layer(Base on Figure 5) introduced by GoogleNet V4\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> torch.nn.Sequential:\n",
    "        \"\"\"\n",
    "        Creates a Inception layer with a custom ReLU activation Function.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels\n",
    "            out_channels (int): Number of output channels\n",
    "            The value of the out_channels parameter must always be a multiple of 8.\n",
    "        Returns:\n",
    "            torch.nn.Sequential: A sequential block representing the Inception_B Layer\n",
    "        \"\"\"\n",
    "        super(Inception_B, self).__init__()\n",
    "\n",
    "        self.branch1_1 = torch.nn.AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
    "        self.branch1_2 = ConvolutionalLayer.create(in_channels, (out_channels // 8) * 1, 1, 1, 0)\n",
    "\n",
    "        self.branch2_1 = ConvolutionalLayer.create(in_channels, (out_channels // 8) * 3, 1, 1, 0)\n",
    "\n",
    "        self.branch3_1 = ConvolutionalLayer.create(in_channels, 192, 1, 1, 0)\n",
    "        self.branch3_2 = ConvolutionalLayer.create(192, 224, (1, 7), 1, (0, 3))\n",
    "        self.branch3_3 = ConvolutionalLayer.create(224, (out_channels // 8) * 2, (7, 1), 1, (3, 0))\n",
    "\n",
    "        self.branch4_1 = ConvolutionalLayer.create(in_channels, 192, 1, 1, 0)\n",
    "        self.branch4_2 = ConvolutionalLayer.create(192, 192, (1, 7), 1, (0, 3))\n",
    "        self.branch4_3 = ConvolutionalLayer.create(192, 224, (7, 1), 1, (3, 0))\n",
    "        self.branch4_4 = ConvolutionalLayer.create(224, 224, (1, 7), 1, (0, 3))\n",
    "        self.branch4_5 = ConvolutionalLayer.create(224, (out_channels // 8) * 2, (7, 1), 1, (3, 0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the InceptionV1 layer.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width)\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after passing through all branches of the Inception_B layer\n",
    "        \"\"\"\n",
    "        x_b1 = self.branch1_1(x)\n",
    "        x_b1 = self.branch1_2(x_b1)\n",
    "\n",
    "        x_b2 = self.branch2_1(x)\n",
    "\n",
    "        x_b3 = self.branch3_1(x)\n",
    "        x_b3 = self.branch3_2(x_b3)\n",
    "        x_b3 = self.branch3_3(x_b3)\n",
    "\n",
    "        x_b4 = self.branch4_1(x)\n",
    "        x_b4 = self.branch4_2(x_b4)\n",
    "        x_b4 = self.branch4_3(x_b4)\n",
    "        x_b4 = self.branch4_4(x_b4)\n",
    "        x_b4 = self.branch4_5(x_b4)\n",
    "\n",
    "        x = torch.cat([x_b1, x_b2, x_b3, x_b4], dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cba988ff-f4c4-47a6-889e-3fc38889b665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Inception_C(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the Inception_C layer(Base on Figure 6) introduced by GoogleNet V4\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> torch.nn.Sequential:\n",
    "        \"\"\"\n",
    "        Creates a Inception layer with a custom ReLU activation Function.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels\n",
    "            out_channels (int): Number of output channels\n",
    "            The value of the out_channels parameter must always be a multiple of 6.\n",
    "        Returns:\n",
    "            torch.nn.Sequential: A sequential block representing the Inception_C Layer\n",
    "        \"\"\"\n",
    "        super(Inception_C, self).__init__()\n",
    "\n",
    "        self.branch1_1 = torch.nn.AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
    "        self.branch1_2 = ConvolutionalLayer.create(in_channels, out_channels // 6, 1, 1, 0)\n",
    "\n",
    "        self.branch2_1 = ConvolutionalLayer.create(in_channels, out_channels // 6, 1, 1, 0)\n",
    "\n",
    "        self.branch3_1 = ConvolutionalLayer.create(in_channels, 384, 1, 1, 0)\n",
    "        self.branch3_2a = ConvolutionalLayer.create(384, out_channels // 6, (1, 3), 1, (0, 1))\n",
    "        self.branch3_2b = ConvolutionalLayer.create(384, out_channels // 6, (3, 1), 1, (1, 0))\n",
    "\n",
    "        self.branch4_1 = ConvolutionalLayer.create(in_channels, 384, 1, 1, 0)\n",
    "        self.branch4_2 = ConvolutionalLayer.create(384, 448, (1, 3), 1, (0, 1))\n",
    "        self.branch4_3 = ConvolutionalLayer.create(448, 512, (3, 1), 1, (1, 0))\n",
    "        self.branch4_4a = ConvolutionalLayer.create(512, out_channels // 6, (3, 1), 1, (1, 0))\n",
    "        self.branch4_4b = ConvolutionalLayer.create(512, out_channels // 6, (1, 3), 1, (0, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the InceptionV1 layer.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width)\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after passing through all branches of the Inception_C layer\n",
    "        \"\"\"\n",
    "        x_b1 = self.branch1_1(x)\n",
    "        x_b1 = self.branch1_2(x_b1)\n",
    "\n",
    "        x_b2 = self.branch2_1(x)\n",
    "\n",
    "        x_b3 = self.branch3_1(x)\n",
    "        x_b3_a = self.branch3_2a(x_b3)\n",
    "        x_b3_b = self.branch3_2b(x_b3)\n",
    "        \n",
    "        x_b4 = self.branch4_1(x)\n",
    "        x_b4 = self.branch4_2(x_b4)\n",
    "        x_b4 = self.branch4_3(x_b4)\n",
    "        x_b4_a = self.branch4_4a(x_b4)\n",
    "        x_b4_b = self.branch4_4b(x_b4)\n",
    "\n",
    "        x = torch.cat([x_b1, x_b2, x_b3_a, x_b3_b, x_b4_a, x_b4_b], dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "615f70b0-5174-4081-9d13-7e5a0ba875e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Reduction_A(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the Reduction_A layer(Base on Figure 7) introduced by GoogleNet V4\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, n: int, k: int, l: int, m: int) -> torch.nn.Sequential:\n",
    "        \"\"\"\n",
    "        Creates a Inception layer with a custom ReLU activation Function.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels\n",
    "            n (int): The number of filters to output after the 3x3 Convolutions operation in Branch2. (In Paper, 384)\n",
    "            k (int): The number of filters to output after the 1x1 Convolutions operation in Branch3. (In Paper, 192)\n",
    "            l (int): The number of filters to output after the 3x3 Convolutions operation in Branch3. (In Paper, 224)\n",
    "            m (int): The number of filters to output after the 3x3 Convolutions operation(with stride=2) in Branch3. (In Paper, 256)\n",
    "        Returns:\n",
    "            torch.nn.Sequential: A sequential block representing the Reduction_A Layer\n",
    "        \"\"\"\n",
    "        super(Reduction_A, self).__init__()\n",
    "\n",
    "        self.branch1_1 = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "\n",
    "        self.branch2_1 = ConvolutionalLayer.create(in_channels, n, 3, 2, 0)\n",
    "\n",
    "        self.branch3_1 = ConvolutionalLayer.create(in_channels, k, 1, 1, 0)\n",
    "        self.branch3_2 = ConvolutionalLayer.create(k, l, 3, 1, 1)\n",
    "        self.branch3_3 = ConvolutionalLayer.create(l, m, 3, 2, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the InceptionV1 layer.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width)\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after passing through all branches of the Reduction_A layer\n",
    "        \"\"\"\n",
    "        x_b1 = self.branch1_1(x)\n",
    "\n",
    "        x_b2 = self.branch2_1(x)\n",
    "\n",
    "        x_b3 = self.branch3_1(x)\n",
    "        x_b3 = self.branch3_2(x_b3)\n",
    "        x_b3 = self.branch3_3(x_b3)\n",
    "\n",
    "        x = torch.cat([x_b1, x_b2, x_b3], dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28559752-8fd2-43fc-99cf-42deed9b5697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Reduction_B(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the Reduction_B layer(Base on Figure 8) introduced by GoogleNet V4\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> torch.nn.Sequential:\n",
    "        \"\"\"\n",
    "        Creates a Inception layer with a custom ReLU activation Function.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels\n",
    "            out_channels (int): Number of output channels\n",
    "            The value of the out_channels parameter must always be a multiple of 24.\n",
    "        Returns:\n",
    "            torch.nn.Sequential: A sequential block representing the Reduction_B Layer\n",
    "        \"\"\"\n",
    "        super(Reduction_B, self).__init__()\n",
    "\n",
    "        self.branch1_1 = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "\n",
    "        self.branch2_1 = ConvolutionalLayer.create(in_channels, 192, 1, 1, 0)\n",
    "        self.branch2_2 = ConvolutionalLayer.create(192, (out_channels // 24) * 3, 3, 2, 0)\n",
    "\n",
    "        self.branch3_1 = ConvolutionalLayer.create(in_channels, 256, 1, 1, 0)\n",
    "        self.branch3_2 = ConvolutionalLayer.create(256, 256, (1, 7), 1, (0, 3))\n",
    "        self.branch3_3 = ConvolutionalLayer.create(256, 320, (7, 1), 1, (3, 0))\n",
    "        self.branch3_4 = ConvolutionalLayer.create(320, (out_channels // 24) * 5, 3, 2, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the InceptionV1 layer.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width)\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after passing through all branches of the Reduction_B layer\n",
    "        \"\"\"\n",
    "        x_b1 = self.branch1_1(x)\n",
    "\n",
    "        x_b2 = self.branch2_1(x)\n",
    "        x_b2 = self.branch2_2(x_b2)\n",
    "\n",
    "        x_b3 = self.branch3_1(x)\n",
    "        x_b3 = self.branch3_2(x_b3)\n",
    "        x_b3 = self.branch3_3(x_b3)\n",
    "        x_b3 = self.branch3_4(x_b3)\n",
    "\n",
    "        x = torch.cat([x_b1, x_b2, x_b3], dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05e28546-0cde-424e-b1b0-ee510af9e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CustomGoogLeNet_V4(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GoogLeNet_V4 model.\n",
    "    Input: Image tensor (batch_size, 3, 299, 299)\n",
    "    Output: Class scores (batch_size, 1000)\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_rate=0.8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dropout_rate (float): Dropout Rate = 0.8(Base on Paper)\n",
    "        \"\"\"\n",
    "        super(CustomGoogLeNet_V4, self).__init__()\n",
    "\n",
    "        # Convolutional and pooling layers\n",
    "        self.layer1 = Stem(3, 384)\n",
    "\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            *[Inception_A(384, 384) for _ in range(4)]\n",
    "        )\n",
    "\n",
    "        self.layer3 = Reduction_A(384, 384, 192, 224, 256)\n",
    "\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            *[Inception_B(1024, 1024) for _ in range(7)]\n",
    "        )\n",
    "\n",
    "        self.layer5 = Reduction_B(1024, 1536)\n",
    "\n",
    "        self.layer6 = torch.nn.Sequential(\n",
    "            *[Inception_C(1536, 1536) for _ in range(3)]\n",
    "        )\n",
    "\n",
    "        self.layer7 = torch.nn.AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
    "        \n",
    "\n",
    "        # Fully Connected layers and dropout\n",
    "        self.layer_drop = torch.nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        self.layer8 = FullyConnectedLayer.Dense(1536, 1000)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input image tensor (batch_size, 3, 299, 299)\n",
    "        Returns:\n",
    "            torch.Tensor: Class scores (batch_size, 1000)\n",
    "        \"\"\"\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "        x = self.layer7(x)\n",
    "        x = self.layer_drop(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.layer8(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dde87c1a-4705-4945-ba54-bb40126f0140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomGoogLeNet_V4\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 149, 149]             896\n",
      "       BatchNorm2d-2         [-1, 32, 149, 149]              64\n",
      "        CustomReLU-3         [-1, 32, 149, 149]               0\n",
      "            Conv2d-4         [-1, 32, 147, 147]           9,248\n",
      "       BatchNorm2d-5         [-1, 32, 147, 147]              64\n",
      "        CustomReLU-6         [-1, 32, 147, 147]               0\n",
      "            Conv2d-7         [-1, 64, 147, 147]          18,496\n",
      "       BatchNorm2d-8         [-1, 64, 147, 147]             128\n",
      "        CustomReLU-9         [-1, 64, 147, 147]               0\n",
      "        MaxPool2d-10           [-1, 64, 73, 73]               0\n",
      "           Conv2d-11           [-1, 96, 73, 73]          55,392\n",
      "      BatchNorm2d-12           [-1, 96, 73, 73]             192\n",
      "       CustomReLU-13           [-1, 96, 73, 73]               0\n",
      "           Conv2d-14           [-1, 64, 73, 73]          10,304\n",
      "      BatchNorm2d-15           [-1, 64, 73, 73]             128\n",
      "       CustomReLU-16           [-1, 64, 73, 73]               0\n",
      "           Conv2d-17           [-1, 96, 71, 71]          55,392\n",
      "      BatchNorm2d-18           [-1, 96, 71, 71]             192\n",
      "       CustomReLU-19           [-1, 96, 71, 71]               0\n",
      "           Conv2d-20           [-1, 64, 73, 73]          10,304\n",
      "      BatchNorm2d-21           [-1, 64, 73, 73]             128\n",
      "       CustomReLU-22           [-1, 64, 73, 73]               0\n",
      "           Conv2d-23           [-1, 64, 73, 73]          28,736\n",
      "      BatchNorm2d-24           [-1, 64, 73, 73]             128\n",
      "       CustomReLU-25           [-1, 64, 73, 73]               0\n",
      "           Conv2d-26           [-1, 64, 73, 73]          28,736\n",
      "      BatchNorm2d-27           [-1, 64, 73, 73]             128\n",
      "       CustomReLU-28           [-1, 64, 73, 73]               0\n",
      "           Conv2d-29           [-1, 96, 71, 71]          55,392\n",
      "      BatchNorm2d-30           [-1, 96, 71, 71]             192\n",
      "       CustomReLU-31           [-1, 96, 71, 71]               0\n",
      "           Conv2d-32          [-1, 192, 35, 35]         331,968\n",
      "      BatchNorm2d-33          [-1, 192, 35, 35]             384\n",
      "       CustomReLU-34          [-1, 192, 35, 35]               0\n",
      "        MaxPool2d-35          [-1, 192, 35, 35]               0\n",
      "             Stem-36          [-1, 384, 35, 35]               0\n",
      "        AvgPool2d-37          [-1, 384, 35, 35]               0\n",
      "           Conv2d-38           [-1, 96, 35, 35]          36,960\n",
      "      BatchNorm2d-39           [-1, 96, 35, 35]             192\n",
      "       CustomReLU-40           [-1, 96, 35, 35]               0\n",
      "           Conv2d-41           [-1, 96, 35, 35]          36,960\n",
      "      BatchNorm2d-42           [-1, 96, 35, 35]             192\n",
      "       CustomReLU-43           [-1, 96, 35, 35]               0\n",
      "           Conv2d-44           [-1, 64, 35, 35]          24,640\n",
      "      BatchNorm2d-45           [-1, 64, 35, 35]             128\n",
      "       CustomReLU-46           [-1, 64, 35, 35]               0\n",
      "           Conv2d-47           [-1, 96, 35, 35]          55,392\n",
      "      BatchNorm2d-48           [-1, 96, 35, 35]             192\n",
      "       CustomReLU-49           [-1, 96, 35, 35]               0\n",
      "           Conv2d-50           [-1, 64, 35, 35]          24,640\n",
      "      BatchNorm2d-51           [-1, 64, 35, 35]             128\n",
      "       CustomReLU-52           [-1, 64, 35, 35]               0\n",
      "           Conv2d-53           [-1, 96, 35, 35]          55,392\n",
      "      BatchNorm2d-54           [-1, 96, 35, 35]             192\n",
      "       CustomReLU-55           [-1, 96, 35, 35]               0\n",
      "           Conv2d-56           [-1, 96, 35, 35]          83,040\n",
      "      BatchNorm2d-57           [-1, 96, 35, 35]             192\n",
      "       CustomReLU-58           [-1, 96, 35, 35]               0\n",
      "      Inception_A-59          [-1, 384, 35, 35]               0\n",
      "        AvgPool2d-60          [-1, 384, 35, 35]               0\n",
      "           Conv2d-61           [-1, 96, 35, 35]          36,960\n",
      "      BatchNorm2d-62           [-1, 96, 35, 35]             192\n",
      "       CustomReLU-63           [-1, 96, 35, 35]               0\n",
      "           Conv2d-64           [-1, 96, 35, 35]          36,960\n",
      "      BatchNorm2d-65           [-1, 96, 35, 35]             192\n",
      "       CustomReLU-66           [-1, 96, 35, 35]               0\n",
      "           Conv2d-67           [-1, 64, 35, 35]          24,640\n",
      "      BatchNorm2d-68           [-1, 64, 35, 35]             128\n",
      "       CustomReLU-69           [-1, 64, 35, 35]               0\n",
      "           Conv2d-70           [-1, 96, 35, 35]          55,392\n",
      "      BatchNorm2d-71           [-1, 96, 35, 35]             192\n",
      "       CustomReLU-72           [-1, 96, 35, 35]               0\n",
      "           Conv2d-73           [-1, 64, 35, 35]          24,640\n",
      "      BatchNorm2d-74           [-1, 64, 35, 35]             128\n",
      "       CustomReLU-75           [-1, 64, 35, 35]               0\n",
      "           Conv2d-76           [-1, 96, 35, 35]          55,392\n",
      "      BatchNorm2d-77           [-1, 96, 35, 35]             192\n",
      "       CustomReLU-78           [-1, 96, 35, 35]               0\n",
      "           Conv2d-79           [-1, 96, 35, 35]          83,040\n",
      "      BatchNorm2d-80           [-1, 96, 35, 35]             192\n",
      "       CustomReLU-81           [-1, 96, 35, 35]               0\n",
      "      Inception_A-82          [-1, 384, 35, 35]               0\n",
      "        AvgPool2d-83          [-1, 384, 35, 35]               0\n",
      "           Conv2d-84           [-1, 96, 35, 35]          36,960\n",
      "      BatchNorm2d-85           [-1, 96, 35, 35]             192\n",
      "       CustomReLU-86           [-1, 96, 35, 35]               0\n",
      "           Conv2d-87           [-1, 96, 35, 35]          36,960\n",
      "      BatchNorm2d-88           [-1, 96, 35, 35]             192\n",
      "       CustomReLU-89           [-1, 96, 35, 35]               0\n",
      "           Conv2d-90           [-1, 64, 35, 35]          24,640\n",
      "      BatchNorm2d-91           [-1, 64, 35, 35]             128\n",
      "       CustomReLU-92           [-1, 64, 35, 35]               0\n",
      "           Conv2d-93           [-1, 96, 35, 35]          55,392\n",
      "      BatchNorm2d-94           [-1, 96, 35, 35]             192\n",
      "       CustomReLU-95           [-1, 96, 35, 35]               0\n",
      "           Conv2d-96           [-1, 64, 35, 35]          24,640\n",
      "      BatchNorm2d-97           [-1, 64, 35, 35]             128\n",
      "       CustomReLU-98           [-1, 64, 35, 35]               0\n",
      "           Conv2d-99           [-1, 96, 35, 35]          55,392\n",
      "     BatchNorm2d-100           [-1, 96, 35, 35]             192\n",
      "      CustomReLU-101           [-1, 96, 35, 35]               0\n",
      "          Conv2d-102           [-1, 96, 35, 35]          83,040\n",
      "     BatchNorm2d-103           [-1, 96, 35, 35]             192\n",
      "      CustomReLU-104           [-1, 96, 35, 35]               0\n",
      "     Inception_A-105          [-1, 384, 35, 35]               0\n",
      "       AvgPool2d-106          [-1, 384, 35, 35]               0\n",
      "          Conv2d-107           [-1, 96, 35, 35]          36,960\n",
      "     BatchNorm2d-108           [-1, 96, 35, 35]             192\n",
      "      CustomReLU-109           [-1, 96, 35, 35]               0\n",
      "          Conv2d-110           [-1, 96, 35, 35]          36,960\n",
      "     BatchNorm2d-111           [-1, 96, 35, 35]             192\n",
      "      CustomReLU-112           [-1, 96, 35, 35]               0\n",
      "          Conv2d-113           [-1, 64, 35, 35]          24,640\n",
      "     BatchNorm2d-114           [-1, 64, 35, 35]             128\n",
      "      CustomReLU-115           [-1, 64, 35, 35]               0\n",
      "          Conv2d-116           [-1, 96, 35, 35]          55,392\n",
      "     BatchNorm2d-117           [-1, 96, 35, 35]             192\n",
      "      CustomReLU-118           [-1, 96, 35, 35]               0\n",
      "          Conv2d-119           [-1, 64, 35, 35]          24,640\n",
      "     BatchNorm2d-120           [-1, 64, 35, 35]             128\n",
      "      CustomReLU-121           [-1, 64, 35, 35]               0\n",
      "          Conv2d-122           [-1, 96, 35, 35]          55,392\n",
      "     BatchNorm2d-123           [-1, 96, 35, 35]             192\n",
      "      CustomReLU-124           [-1, 96, 35, 35]               0\n",
      "          Conv2d-125           [-1, 96, 35, 35]          83,040\n",
      "     BatchNorm2d-126           [-1, 96, 35, 35]             192\n",
      "      CustomReLU-127           [-1, 96, 35, 35]               0\n",
      "     Inception_A-128          [-1, 384, 35, 35]               0\n",
      "       MaxPool2d-129          [-1, 384, 17, 17]               0\n",
      "          Conv2d-130          [-1, 384, 17, 17]       1,327,488\n",
      "     BatchNorm2d-131          [-1, 384, 17, 17]             768\n",
      "      CustomReLU-132          [-1, 384, 17, 17]               0\n",
      "          Conv2d-133          [-1, 192, 35, 35]          73,920\n",
      "     BatchNorm2d-134          [-1, 192, 35, 35]             384\n",
      "      CustomReLU-135          [-1, 192, 35, 35]               0\n",
      "          Conv2d-136          [-1, 224, 35, 35]         387,296\n",
      "     BatchNorm2d-137          [-1, 224, 35, 35]             448\n",
      "      CustomReLU-138          [-1, 224, 35, 35]               0\n",
      "          Conv2d-139          [-1, 256, 17, 17]         516,352\n",
      "     BatchNorm2d-140          [-1, 256, 17, 17]             512\n",
      "      CustomReLU-141          [-1, 256, 17, 17]               0\n",
      "     Reduction_A-142         [-1, 1024, 17, 17]               0\n",
      "       AvgPool2d-143         [-1, 1024, 17, 17]               0\n",
      "          Conv2d-144          [-1, 128, 17, 17]         131,200\n",
      "     BatchNorm2d-145          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-146          [-1, 128, 17, 17]               0\n",
      "          Conv2d-147          [-1, 384, 17, 17]         393,600\n",
      "     BatchNorm2d-148          [-1, 384, 17, 17]             768\n",
      "      CustomReLU-149          [-1, 384, 17, 17]               0\n",
      "          Conv2d-150          [-1, 192, 17, 17]         196,800\n",
      "     BatchNorm2d-151          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-152          [-1, 192, 17, 17]               0\n",
      "          Conv2d-153          [-1, 224, 17, 17]         301,280\n",
      "     BatchNorm2d-154          [-1, 224, 17, 17]             448\n",
      "      CustomReLU-155          [-1, 224, 17, 17]               0\n",
      "          Conv2d-156          [-1, 256, 17, 17]         401,664\n",
      "     BatchNorm2d-157          [-1, 256, 17, 17]             512\n",
      "      CustomReLU-158          [-1, 256, 17, 17]               0\n",
      "          Conv2d-159          [-1, 192, 17, 17]         196,800\n",
      "     BatchNorm2d-160          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-161          [-1, 192, 17, 17]               0\n",
      "          Conv2d-162          [-1, 192, 17, 17]         258,240\n",
      "     BatchNorm2d-163          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-164          [-1, 192, 17, 17]               0\n",
      "          Conv2d-165          [-1, 224, 17, 17]         301,280\n",
      "     BatchNorm2d-166          [-1, 224, 17, 17]             448\n",
      "      CustomReLU-167          [-1, 224, 17, 17]               0\n",
      "          Conv2d-168          [-1, 224, 17, 17]         351,456\n",
      "     BatchNorm2d-169          [-1, 224, 17, 17]             448\n",
      "      CustomReLU-170          [-1, 224, 17, 17]               0\n",
      "          Conv2d-171          [-1, 256, 17, 17]         401,664\n",
      "     BatchNorm2d-172          [-1, 256, 17, 17]             512\n",
      "      CustomReLU-173          [-1, 256, 17, 17]               0\n",
      "     Inception_B-174         [-1, 1024, 17, 17]               0\n",
      "       AvgPool2d-175         [-1, 1024, 17, 17]               0\n",
      "          Conv2d-176          [-1, 128, 17, 17]         131,200\n",
      "     BatchNorm2d-177          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-178          [-1, 128, 17, 17]               0\n",
      "          Conv2d-179          [-1, 384, 17, 17]         393,600\n",
      "     BatchNorm2d-180          [-1, 384, 17, 17]             768\n",
      "      CustomReLU-181          [-1, 384, 17, 17]               0\n",
      "          Conv2d-182          [-1, 192, 17, 17]         196,800\n",
      "     BatchNorm2d-183          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-184          [-1, 192, 17, 17]               0\n",
      "          Conv2d-185          [-1, 224, 17, 17]         301,280\n",
      "     BatchNorm2d-186          [-1, 224, 17, 17]             448\n",
      "      CustomReLU-187          [-1, 224, 17, 17]               0\n",
      "          Conv2d-188          [-1, 256, 17, 17]         401,664\n",
      "     BatchNorm2d-189          [-1, 256, 17, 17]             512\n",
      "      CustomReLU-190          [-1, 256, 17, 17]               0\n",
      "          Conv2d-191          [-1, 192, 17, 17]         196,800\n",
      "     BatchNorm2d-192          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-193          [-1, 192, 17, 17]               0\n",
      "          Conv2d-194          [-1, 192, 17, 17]         258,240\n",
      "     BatchNorm2d-195          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-196          [-1, 192, 17, 17]               0\n",
      "          Conv2d-197          [-1, 224, 17, 17]         301,280\n",
      "     BatchNorm2d-198          [-1, 224, 17, 17]             448\n",
      "      CustomReLU-199          [-1, 224, 17, 17]               0\n",
      "          Conv2d-200          [-1, 224, 17, 17]         351,456\n",
      "     BatchNorm2d-201          [-1, 224, 17, 17]             448\n",
      "      CustomReLU-202          [-1, 224, 17, 17]               0\n",
      "          Conv2d-203          [-1, 256, 17, 17]         401,664\n",
      "     BatchNorm2d-204          [-1, 256, 17, 17]             512\n",
      "      CustomReLU-205          [-1, 256, 17, 17]               0\n",
      "     Inception_B-206         [-1, 1024, 17, 17]               0\n",
      "       AvgPool2d-207         [-1, 1024, 17, 17]               0\n",
      "          Conv2d-208          [-1, 128, 17, 17]         131,200\n",
      "     BatchNorm2d-209          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-210          [-1, 128, 17, 17]               0\n",
      "          Conv2d-211          [-1, 384, 17, 17]         393,600\n",
      "     BatchNorm2d-212          [-1, 384, 17, 17]             768\n",
      "      CustomReLU-213          [-1, 384, 17, 17]               0\n",
      "          Conv2d-214          [-1, 192, 17, 17]         196,800\n",
      "     BatchNorm2d-215          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-216          [-1, 192, 17, 17]               0\n",
      "          Conv2d-217          [-1, 224, 17, 17]         301,280\n",
      "     BatchNorm2d-218          [-1, 224, 17, 17]             448\n",
      "      CustomReLU-219          [-1, 224, 17, 17]               0\n",
      "          Conv2d-220          [-1, 256, 17, 17]         401,664\n",
      "     BatchNorm2d-221          [-1, 256, 17, 17]             512\n",
      "      CustomReLU-222          [-1, 256, 17, 17]               0\n",
      "          Conv2d-223          [-1, 192, 17, 17]         196,800\n",
      "     BatchNorm2d-224          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-225          [-1, 192, 17, 17]               0\n",
      "          Conv2d-226          [-1, 192, 17, 17]         258,240\n",
      "     BatchNorm2d-227          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-228          [-1, 192, 17, 17]               0\n",
      "          Conv2d-229          [-1, 224, 17, 17]         301,280\n",
      "     BatchNorm2d-230          [-1, 224, 17, 17]             448\n",
      "      CustomReLU-231          [-1, 224, 17, 17]               0\n",
      "          Conv2d-232          [-1, 224, 17, 17]         351,456\n",
      "     BatchNorm2d-233          [-1, 224, 17, 17]             448\n",
      "      CustomReLU-234          [-1, 224, 17, 17]               0\n",
      "          Conv2d-235          [-1, 256, 17, 17]         401,664\n",
      "     BatchNorm2d-236          [-1, 256, 17, 17]             512\n",
      "      CustomReLU-237          [-1, 256, 17, 17]               0\n",
      "     Inception_B-238         [-1, 1024, 17, 17]               0\n",
      "       AvgPool2d-239         [-1, 1024, 17, 17]               0\n",
      "          Conv2d-240          [-1, 128, 17, 17]         131,200\n",
      "     BatchNorm2d-241          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-242          [-1, 128, 17, 17]               0\n",
      "          Conv2d-243          [-1, 384, 17, 17]         393,600\n",
      "     BatchNorm2d-244          [-1, 384, 17, 17]             768\n",
      "      CustomReLU-245          [-1, 384, 17, 17]               0\n",
      "          Conv2d-246          [-1, 192, 17, 17]         196,800\n",
      "     BatchNorm2d-247          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-248          [-1, 192, 17, 17]               0\n",
      "          Conv2d-249          [-1, 224, 17, 17]         301,280\n",
      "     BatchNorm2d-250          [-1, 224, 17, 17]             448\n",
      "      CustomReLU-251          [-1, 224, 17, 17]               0\n",
      "          Conv2d-252          [-1, 256, 17, 17]         401,664\n",
      "     BatchNorm2d-253          [-1, 256, 17, 17]             512\n",
      "      CustomReLU-254          [-1, 256, 17, 17]               0\n",
      "          Conv2d-255          [-1, 192, 17, 17]         196,800\n",
      "     BatchNorm2d-256          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-257          [-1, 192, 17, 17]               0\n",
      "          Conv2d-258          [-1, 192, 17, 17]         258,240\n",
      "     BatchNorm2d-259          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-260          [-1, 192, 17, 17]               0\n",
      "          Conv2d-261          [-1, 224, 17, 17]         301,280\n",
      "     BatchNorm2d-262          [-1, 224, 17, 17]             448\n",
      "      CustomReLU-263          [-1, 224, 17, 17]               0\n",
      "          Conv2d-264          [-1, 224, 17, 17]         351,456\n",
      "     BatchNorm2d-265          [-1, 224, 17, 17]             448\n",
      "      CustomReLU-266          [-1, 224, 17, 17]               0\n",
      "          Conv2d-267          [-1, 256, 17, 17]         401,664\n",
      "     BatchNorm2d-268          [-1, 256, 17, 17]             512\n",
      "      CustomReLU-269          [-1, 256, 17, 17]               0\n",
      "     Inception_B-270         [-1, 1024, 17, 17]               0\n",
      "       AvgPool2d-271         [-1, 1024, 17, 17]               0\n",
      "          Conv2d-272          [-1, 128, 17, 17]         131,200\n",
      "     BatchNorm2d-273          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-274          [-1, 128, 17, 17]               0\n",
      "          Conv2d-275          [-1, 384, 17, 17]         393,600\n",
      "     BatchNorm2d-276          [-1, 384, 17, 17]             768\n",
      "      CustomReLU-277          [-1, 384, 17, 17]               0\n",
      "          Conv2d-278          [-1, 192, 17, 17]         196,800\n",
      "     BatchNorm2d-279          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-280          [-1, 192, 17, 17]               0\n",
      "          Conv2d-281          [-1, 224, 17, 17]         301,280\n",
      "     BatchNorm2d-282          [-1, 224, 17, 17]             448\n",
      "      CustomReLU-283          [-1, 224, 17, 17]               0\n",
      "          Conv2d-284          [-1, 256, 17, 17]         401,664\n",
      "     BatchNorm2d-285          [-1, 256, 17, 17]             512\n",
      "      CustomReLU-286          [-1, 256, 17, 17]               0\n",
      "          Conv2d-287          [-1, 192, 17, 17]         196,800\n",
      "     BatchNorm2d-288          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-289          [-1, 192, 17, 17]               0\n",
      "          Conv2d-290          [-1, 192, 17, 17]         258,240\n",
      "     BatchNorm2d-291          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-292          [-1, 192, 17, 17]               0\n",
      "          Conv2d-293          [-1, 224, 17, 17]         301,280\n",
      "     BatchNorm2d-294          [-1, 224, 17, 17]             448\n",
      "      CustomReLU-295          [-1, 224, 17, 17]               0\n",
      "          Conv2d-296          [-1, 224, 17, 17]         351,456\n",
      "     BatchNorm2d-297          [-1, 224, 17, 17]             448\n",
      "      CustomReLU-298          [-1, 224, 17, 17]               0\n",
      "          Conv2d-299          [-1, 256, 17, 17]         401,664\n",
      "     BatchNorm2d-300          [-1, 256, 17, 17]             512\n",
      "      CustomReLU-301          [-1, 256, 17, 17]               0\n",
      "     Inception_B-302         [-1, 1024, 17, 17]               0\n",
      "       AvgPool2d-303         [-1, 1024, 17, 17]               0\n",
      "          Conv2d-304          [-1, 128, 17, 17]         131,200\n",
      "     BatchNorm2d-305          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-306          [-1, 128, 17, 17]               0\n",
      "          Conv2d-307          [-1, 384, 17, 17]         393,600\n",
      "     BatchNorm2d-308          [-1, 384, 17, 17]             768\n",
      "      CustomReLU-309          [-1, 384, 17, 17]               0\n",
      "          Conv2d-310          [-1, 192, 17, 17]         196,800\n",
      "     BatchNorm2d-311          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-312          [-1, 192, 17, 17]               0\n",
      "          Conv2d-313          [-1, 224, 17, 17]         301,280\n",
      "     BatchNorm2d-314          [-1, 224, 17, 17]             448\n",
      "      CustomReLU-315          [-1, 224, 17, 17]               0\n",
      "          Conv2d-316          [-1, 256, 17, 17]         401,664\n",
      "     BatchNorm2d-317          [-1, 256, 17, 17]             512\n",
      "      CustomReLU-318          [-1, 256, 17, 17]               0\n",
      "          Conv2d-319          [-1, 192, 17, 17]         196,800\n",
      "     BatchNorm2d-320          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-321          [-1, 192, 17, 17]               0\n",
      "          Conv2d-322          [-1, 192, 17, 17]         258,240\n",
      "     BatchNorm2d-323          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-324          [-1, 192, 17, 17]               0\n",
      "          Conv2d-325          [-1, 224, 17, 17]         301,280\n",
      "     BatchNorm2d-326          [-1, 224, 17, 17]             448\n",
      "      CustomReLU-327          [-1, 224, 17, 17]               0\n",
      "          Conv2d-328          [-1, 224, 17, 17]         351,456\n",
      "     BatchNorm2d-329          [-1, 224, 17, 17]             448\n",
      "      CustomReLU-330          [-1, 224, 17, 17]               0\n",
      "          Conv2d-331          [-1, 256, 17, 17]         401,664\n",
      "     BatchNorm2d-332          [-1, 256, 17, 17]             512\n",
      "      CustomReLU-333          [-1, 256, 17, 17]               0\n",
      "     Inception_B-334         [-1, 1024, 17, 17]               0\n",
      "       AvgPool2d-335         [-1, 1024, 17, 17]               0\n",
      "          Conv2d-336          [-1, 128, 17, 17]         131,200\n",
      "     BatchNorm2d-337          [-1, 128, 17, 17]             256\n",
      "      CustomReLU-338          [-1, 128, 17, 17]               0\n",
      "          Conv2d-339          [-1, 384, 17, 17]         393,600\n",
      "     BatchNorm2d-340          [-1, 384, 17, 17]             768\n",
      "      CustomReLU-341          [-1, 384, 17, 17]               0\n",
      "          Conv2d-342          [-1, 192, 17, 17]         196,800\n",
      "     BatchNorm2d-343          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-344          [-1, 192, 17, 17]               0\n",
      "          Conv2d-345          [-1, 224, 17, 17]         301,280\n",
      "     BatchNorm2d-346          [-1, 224, 17, 17]             448\n",
      "      CustomReLU-347          [-1, 224, 17, 17]               0\n",
      "          Conv2d-348          [-1, 256, 17, 17]         401,664\n",
      "     BatchNorm2d-349          [-1, 256, 17, 17]             512\n",
      "      CustomReLU-350          [-1, 256, 17, 17]               0\n",
      "          Conv2d-351          [-1, 192, 17, 17]         196,800\n",
      "     BatchNorm2d-352          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-353          [-1, 192, 17, 17]               0\n",
      "          Conv2d-354          [-1, 192, 17, 17]         258,240\n",
      "     BatchNorm2d-355          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-356          [-1, 192, 17, 17]               0\n",
      "          Conv2d-357          [-1, 224, 17, 17]         301,280\n",
      "     BatchNorm2d-358          [-1, 224, 17, 17]             448\n",
      "      CustomReLU-359          [-1, 224, 17, 17]               0\n",
      "          Conv2d-360          [-1, 224, 17, 17]         351,456\n",
      "     BatchNorm2d-361          [-1, 224, 17, 17]             448\n",
      "      CustomReLU-362          [-1, 224, 17, 17]               0\n",
      "          Conv2d-363          [-1, 256, 17, 17]         401,664\n",
      "     BatchNorm2d-364          [-1, 256, 17, 17]             512\n",
      "      CustomReLU-365          [-1, 256, 17, 17]               0\n",
      "     Inception_B-366         [-1, 1024, 17, 17]               0\n",
      "       MaxPool2d-367           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-368          [-1, 192, 17, 17]         196,800\n",
      "     BatchNorm2d-369          [-1, 192, 17, 17]             384\n",
      "      CustomReLU-370          [-1, 192, 17, 17]               0\n",
      "          Conv2d-371            [-1, 192, 8, 8]         331,968\n",
      "     BatchNorm2d-372            [-1, 192, 8, 8]             384\n",
      "      CustomReLU-373            [-1, 192, 8, 8]               0\n",
      "          Conv2d-374          [-1, 256, 17, 17]         262,400\n",
      "     BatchNorm2d-375          [-1, 256, 17, 17]             512\n",
      "      CustomReLU-376          [-1, 256, 17, 17]               0\n",
      "          Conv2d-377          [-1, 256, 17, 17]         459,008\n",
      "     BatchNorm2d-378          [-1, 256, 17, 17]             512\n",
      "      CustomReLU-379          [-1, 256, 17, 17]               0\n",
      "          Conv2d-380          [-1, 320, 17, 17]         573,760\n",
      "     BatchNorm2d-381          [-1, 320, 17, 17]             640\n",
      "      CustomReLU-382          [-1, 320, 17, 17]               0\n",
      "          Conv2d-383            [-1, 320, 8, 8]         921,920\n",
      "     BatchNorm2d-384            [-1, 320, 8, 8]             640\n",
      "      CustomReLU-385            [-1, 320, 8, 8]               0\n",
      "     Reduction_B-386           [-1, 1536, 8, 8]               0\n",
      "       AvgPool2d-387           [-1, 1536, 8, 8]               0\n",
      "          Conv2d-388            [-1, 256, 8, 8]         393,472\n",
      "     BatchNorm2d-389            [-1, 256, 8, 8]             512\n",
      "      CustomReLU-390            [-1, 256, 8, 8]               0\n",
      "          Conv2d-391            [-1, 256, 8, 8]         393,472\n",
      "     BatchNorm2d-392            [-1, 256, 8, 8]             512\n",
      "      CustomReLU-393            [-1, 256, 8, 8]               0\n",
      "          Conv2d-394            [-1, 384, 8, 8]         590,208\n",
      "     BatchNorm2d-395            [-1, 384, 8, 8]             768\n",
      "      CustomReLU-396            [-1, 384, 8, 8]               0\n",
      "          Conv2d-397            [-1, 256, 8, 8]         295,168\n",
      "     BatchNorm2d-398            [-1, 256, 8, 8]             512\n",
      "      CustomReLU-399            [-1, 256, 8, 8]               0\n",
      "          Conv2d-400            [-1, 256, 8, 8]         295,168\n",
      "     BatchNorm2d-401            [-1, 256, 8, 8]             512\n",
      "      CustomReLU-402            [-1, 256, 8, 8]               0\n",
      "          Conv2d-403            [-1, 384, 8, 8]         590,208\n",
      "     BatchNorm2d-404            [-1, 384, 8, 8]             768\n",
      "      CustomReLU-405            [-1, 384, 8, 8]               0\n",
      "          Conv2d-406            [-1, 448, 8, 8]         516,544\n",
      "     BatchNorm2d-407            [-1, 448, 8, 8]             896\n",
      "      CustomReLU-408            [-1, 448, 8, 8]               0\n",
      "          Conv2d-409            [-1, 512, 8, 8]         688,640\n",
      "     BatchNorm2d-410            [-1, 512, 8, 8]           1,024\n",
      "      CustomReLU-411            [-1, 512, 8, 8]               0\n",
      "          Conv2d-412            [-1, 256, 8, 8]         393,472\n",
      "     BatchNorm2d-413            [-1, 256, 8, 8]             512\n",
      "      CustomReLU-414            [-1, 256, 8, 8]               0\n",
      "          Conv2d-415            [-1, 256, 8, 8]         393,472\n",
      "     BatchNorm2d-416            [-1, 256, 8, 8]             512\n",
      "      CustomReLU-417            [-1, 256, 8, 8]               0\n",
      "     Inception_C-418           [-1, 1536, 8, 8]               0\n",
      "       AvgPool2d-419           [-1, 1536, 8, 8]               0\n",
      "          Conv2d-420            [-1, 256, 8, 8]         393,472\n",
      "     BatchNorm2d-421            [-1, 256, 8, 8]             512\n",
      "      CustomReLU-422            [-1, 256, 8, 8]               0\n",
      "          Conv2d-423            [-1, 256, 8, 8]         393,472\n",
      "     BatchNorm2d-424            [-1, 256, 8, 8]             512\n",
      "      CustomReLU-425            [-1, 256, 8, 8]               0\n",
      "          Conv2d-426            [-1, 384, 8, 8]         590,208\n",
      "     BatchNorm2d-427            [-1, 384, 8, 8]             768\n",
      "      CustomReLU-428            [-1, 384, 8, 8]               0\n",
      "          Conv2d-429            [-1, 256, 8, 8]         295,168\n",
      "     BatchNorm2d-430            [-1, 256, 8, 8]             512\n",
      "      CustomReLU-431            [-1, 256, 8, 8]               0\n",
      "          Conv2d-432            [-1, 256, 8, 8]         295,168\n",
      "     BatchNorm2d-433            [-1, 256, 8, 8]             512\n",
      "      CustomReLU-434            [-1, 256, 8, 8]               0\n",
      "          Conv2d-435            [-1, 384, 8, 8]         590,208\n",
      "     BatchNorm2d-436            [-1, 384, 8, 8]             768\n",
      "      CustomReLU-437            [-1, 384, 8, 8]               0\n",
      "          Conv2d-438            [-1, 448, 8, 8]         516,544\n",
      "     BatchNorm2d-439            [-1, 448, 8, 8]             896\n",
      "      CustomReLU-440            [-1, 448, 8, 8]               0\n",
      "          Conv2d-441            [-1, 512, 8, 8]         688,640\n",
      "     BatchNorm2d-442            [-1, 512, 8, 8]           1,024\n",
      "      CustomReLU-443            [-1, 512, 8, 8]               0\n",
      "          Conv2d-444            [-1, 256, 8, 8]         393,472\n",
      "     BatchNorm2d-445            [-1, 256, 8, 8]             512\n",
      "      CustomReLU-446            [-1, 256, 8, 8]               0\n",
      "          Conv2d-447            [-1, 256, 8, 8]         393,472\n",
      "     BatchNorm2d-448            [-1, 256, 8, 8]             512\n",
      "      CustomReLU-449            [-1, 256, 8, 8]               0\n",
      "     Inception_C-450           [-1, 1536, 8, 8]               0\n",
      "       AvgPool2d-451           [-1, 1536, 8, 8]               0\n",
      "          Conv2d-452            [-1, 256, 8, 8]         393,472\n",
      "     BatchNorm2d-453            [-1, 256, 8, 8]             512\n",
      "      CustomReLU-454            [-1, 256, 8, 8]               0\n",
      "          Conv2d-455            [-1, 256, 8, 8]         393,472\n",
      "     BatchNorm2d-456            [-1, 256, 8, 8]             512\n",
      "      CustomReLU-457            [-1, 256, 8, 8]               0\n",
      "          Conv2d-458            [-1, 384, 8, 8]         590,208\n",
      "     BatchNorm2d-459            [-1, 384, 8, 8]             768\n",
      "      CustomReLU-460            [-1, 384, 8, 8]               0\n",
      "          Conv2d-461            [-1, 256, 8, 8]         295,168\n",
      "     BatchNorm2d-462            [-1, 256, 8, 8]             512\n",
      "      CustomReLU-463            [-1, 256, 8, 8]               0\n",
      "          Conv2d-464            [-1, 256, 8, 8]         295,168\n",
      "     BatchNorm2d-465            [-1, 256, 8, 8]             512\n",
      "      CustomReLU-466            [-1, 256, 8, 8]               0\n",
      "          Conv2d-467            [-1, 384, 8, 8]         590,208\n",
      "     BatchNorm2d-468            [-1, 384, 8, 8]             768\n",
      "      CustomReLU-469            [-1, 384, 8, 8]               0\n",
      "          Conv2d-470            [-1, 448, 8, 8]         516,544\n",
      "     BatchNorm2d-471            [-1, 448, 8, 8]             896\n",
      "      CustomReLU-472            [-1, 448, 8, 8]               0\n",
      "          Conv2d-473            [-1, 512, 8, 8]         688,640\n",
      "     BatchNorm2d-474            [-1, 512, 8, 8]           1,024\n",
      "      CustomReLU-475            [-1, 512, 8, 8]               0\n",
      "          Conv2d-476            [-1, 256, 8, 8]         393,472\n",
      "     BatchNorm2d-477            [-1, 256, 8, 8]             512\n",
      "      CustomReLU-478            [-1, 256, 8, 8]               0\n",
      "          Conv2d-479            [-1, 256, 8, 8]         393,472\n",
      "     BatchNorm2d-480            [-1, 256, 8, 8]             512\n",
      "      CustomReLU-481            [-1, 256, 8, 8]               0\n",
      "     Inception_C-482           [-1, 1536, 8, 8]               0\n",
      "       AvgPool2d-483           [-1, 1536, 1, 1]               0\n",
      "         Dropout-484           [-1, 1536, 1, 1]               0\n",
      "          Linear-485                 [-1, 1000]       1,537,000\n",
      "================================================================\n",
      "Total params: 42,711,400\n",
      "Trainable params: 42,711,400\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.02\n",
      "Forward/backward pass size (MB): 422.56\n",
      "Params size (MB): 162.93\n",
      "Estimated Total Size (MB): 586.51\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "model = CustomGoogLeNet_V4()\n",
    "\n",
    "print(model.__class__.__name__)\n",
    "summary(model, input_size=(3, 299, 299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a89acb-254d-45b5-b7c1-27a6f635770b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
